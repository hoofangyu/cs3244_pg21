{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from scipy.sparse import csr_matrix, vstack, hstack\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"train.pkl\").dropna()\n",
    "test = pd.read_pickle(\"test.pkl\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding\n",
    "encoded_train = pd.get_dummies(train, columns = ['weighted_parent_sentiment_score','weighted_comment_sentiment_score'], drop_first=True)\n",
    "encoded_test = pd.get_dummies(test, columns = ['weighted_parent_sentiment_score','weighted_comment_sentiment_score'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parent_tdidf_csr = vstack(encoded_train[\"parent_comment_tdidf\"])\n",
    "test_parent_tdidf_csr = vstack(encoded_test[\"parent_comment_tdidf\"])\n",
    "\n",
    "train_tdidf_csr = vstack(encoded_train[\"comment_tdidf\"])\n",
    "test_tdidf_csr = vstack(encoded_test[\"comment_tdidf\"])\n",
    "\n",
    "train_parent_bow_csr = vstack(encoded_train[\"parent_comment_bow\"])\n",
    "test_parent_bow_csr = vstack(encoded_test[\"parent_comment_bow\"])\n",
    "\n",
    "train_bow_csr = vstack(encoded_train[\"comment_bow\"])\n",
    "test_bow_csr = vstack(encoded_test[\"comment_bow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment', 'author', 'subreddit', 'score', 'ups', 'downs', 'date',\n",
       "       'created_utc', 'parent_comment', 'comment_tokens',\n",
       "       'parent_comment_tokens', 'comment_score', 'parent_comment_score',\n",
       "       'comment_word_count', 'parent_comment_word_count',\n",
       "       'comment_token_count', 'parent_comment_token_count',\n",
       "       'comment_unique_word_count', 'parent_comment_unique_word_count',\n",
       "       'comment_unique_token_count', 'parent_comment_unique_token_count',\n",
       "       'comment_stopword_count', 'parent_comment_stopword_count',\n",
       "       'comment_mean_word_length', 'parent_comment_mean_word_length',\n",
       "       'comment_mean_token_length', 'parent_comment_mean_token_length',\n",
       "       'comment_char_count', 'parent_comment_char_count',\n",
       "       'comment_punctuation_count', 'parent_comment_punctuation_count',\n",
       "       'comment_hashtag_count', 'parent_comment_hashtag_count',\n",
       "       'comment_number_count', 'parent_comment_number_count', 'comment_bow',\n",
       "       'parent_comment_bow', 'comment_tdidf', 'parent_comment_tdidf',\n",
       "       'comment_pos', 'parent_comment_pos', 'label',\n",
       "       'weighted_parent_sentiment_score_neutral',\n",
       "       'weighted_parent_sentiment_score_positive',\n",
       "       'weighted_comment_sentiment_score_neutral',\n",
       "       'weighted_comment_sentiment_score_positive'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_features = ['score','ups','downs',\n",
    "       'comment_word_count','parent_comment_word_count',\n",
    "       'comment_token_count', 'parent_comment_token_count',\n",
    "       'comment_unique_word_count', 'parent_comment_unique_word_count',\n",
    "       'comment_unique_token_count', 'parent_comment_unique_token_count',\n",
    "       'comment_stopword_count', 'parent_comment_stopword_count',\n",
    "       'comment_mean_word_length', 'parent_comment_mean_word_length',\n",
    "       'comment_mean_token_length', 'parent_comment_mean_token_length',\n",
    "       'comment_char_count', 'parent_comment_char_count',\n",
    "       'comment_punctuation_count', 'parent_comment_punctuation_count',\n",
    "       'comment_hashtag_count', 'parent_comment_hashtag_count',\n",
    "       'comment_number_count', 'parent_comment_number_count',\n",
    "       'weighted_parent_sentiment_score_neutral',\n",
    "       'weighted_parent_sentiment_score_positive',\n",
    "       'weighted_comment_sentiment_score_neutral',\n",
    "       'weighted_comment_sentiment_score_positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gen_features = csr_matrix(encoded_train[list_of_features])\n",
    "y_train_LR = encoded_train['label']\n",
    "\n",
    "X_test_gen_features = csr_matrix(encoded_test[list_of_features])\n",
    "y_test_LR = encoded_test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 1: General Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Specific Prep\n",
    "X_train_LR = X_train_gen_features\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean roc_auc: 0.5490533863023862\n",
      "Standard Deviation of roc_auc: 0.001240666211296722\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "cross_val_scores = []\n",
    "for train_index, val_index in kf.split(X_train_LR): \n",
    "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
    "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate roc_auc and store it in the list\n",
    "    roc_auc = roc_auc_score(y_val, y_pred)\n",
    "    cross_val_scores.append(roc_auc)\n",
    "\n",
    "mean_roc_auc = sum(cross_val_scores) / k\n",
    "std_roc_auc = np.std(cross_val_scores)\n",
    "\n",
    "print(f\"Mean roc_auc: {mean_roc_auc}\")\n",
    "print(f\"Standard Deviation of roc_auc: {std_roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 2: Comment BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Specific Prep\n",
    "X_train_LR = train_bow_csr\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean roc_auc: 0.6560392997273248\n",
      "Standard Deviation of roc_auc: 0.002913432480225139\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "cross_val_scores = []\n",
    "for train_index, val_index in kf.split(X_train_LR): \n",
    "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
    "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate roc_auc and store it in the list\n",
    "    roc_auc = roc_auc_score(y_val, y_pred)\n",
    "    cross_val_scores.append(roc_auc)\n",
    "\n",
    "mean_roc_auc = sum(cross_val_scores) / k\n",
    "std_roc_auc = np.std(cross_val_scores)\n",
    "\n",
    "print(f\"Mean roc_auc: {mean_roc_auc}\")\n",
    "print(f\"Standard Deviation of roc_auc: {std_roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 3: Comment TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Specific Prep\n",
    "X_train_LR = train_tdidf_csr\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean roc_auc: 0.6562782159989\n",
      "Standard Deviation of roc_auc: 0.001777626065779237\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "cross_val_scores = []\n",
    "for train_index, val_index in kf.split(X_train_LR): \n",
    "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
    "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate roc_auc and store it in the list\n",
    "    roc_auc = roc_auc_score(y_val, y_pred)\n",
    "    cross_val_scores.append(roc_auc)\n",
    "\n",
    "mean_roc_auc = sum(cross_val_scores) / k\n",
    "std_roc_auc = np.std(cross_val_scores)\n",
    "\n",
    "print(f\"Mean roc_auc: {mean_roc_auc}\")\n",
    "print(f\"Standard Deviation of roc_auc: {std_roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 4: General Features + BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_LR = hstack([X_train_gen_features,train_bow_csr])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean roc_auc: 0.6607948025593808\n",
      "Standard Deviation of roc_auc: 0.0023754493018701664\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "cross_val_scores = []\n",
    "for train_index, val_index in kf.split(X_train_LR): \n",
    "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
    "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate roc_auc and store it in the list\n",
    "    roc_auc = roc_auc_score(y_val, y_pred)\n",
    "    cross_val_scores.append(roc_auc)\n",
    "\n",
    "mean_roc_auc = sum(cross_val_scores) / k\n",
    "std_roc_auc = np.std(cross_val_scores)\n",
    "\n",
    "print(f\"Mean roc_auc: {mean_roc_auc}\")\n",
    "print(f\"Standard Deviation of roc_auc: {std_roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 5: General Features + TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_LR = hstack([X_train_gen_features,train_tdidf_csr])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean roc_auc: 0.6611594393419361\n",
      "Standard Deviation of roc_auc: 0.0020047622857415695\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "cross_val_scores = []\n",
    "for train_index, val_index in kf.split(X_train_LR): \n",
    "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
    "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate roc_auc and store it in the list\n",
    "    roc_auc = roc_auc_score(y_val, y_pred)\n",
    "    cross_val_scores.append(roc_auc)\n",
    "\n",
    "mean_roc_auc = sum(cross_val_scores) / k\n",
    "std_roc_auc = np.std(cross_val_scores)\n",
    "\n",
    "print(f\"Mean roc_auc: {mean_roc_auc}\")\n",
    "print(f\"Standard Deviation of roc_auc: {std_roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 6: Gen Features + Comment TDIDF + Parent TDIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_LR = hstack([X_train_gen_features,train_tdidf_csr,train_parent_tdidf_csr])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean roc_auc: 0.6544354031261035\n",
      "Standard Deviation of roc_auc: 0.003690347356804236\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "cross_val_scores = []\n",
    "for train_index, val_index in kf.split(X_train_LR): \n",
    "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
    "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate roc_auc and store it in the list\n",
    "    roc_auc = roc_auc_score(y_val, y_pred)\n",
    "    cross_val_scores.append(roc_auc)\n",
    "\n",
    "mean_roc_auc = sum(cross_val_scores) / k\n",
    "std_roc_auc = np.std(cross_val_scores)\n",
    "\n",
    "print(f\"Mean roc_auc: {mean_roc_auc}\")\n",
    "print(f\"Standard Deviation of roc_auc: {std_roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 7: Gen + Parent TDIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_LR = hstack([X_train_gen_features,train_parent_tdidf_csr])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean roc_auc: 0.5686531925222138\n",
      "Standard Deviation of roc_auc: 0.001365869275306009\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "cross_val_scores = []\n",
    "for train_index, val_index in kf.split(X_train_LR): \n",
    "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
    "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate roc_auc and store it in the list\n",
    "    roc_auc = roc_auc_score(y_val, y_pred)\n",
    "    cross_val_scores.append(roc_auc)\n",
    "\n",
    "mean_roc_auc = sum(cross_val_scores) / k\n",
    "std_roc_auc = np.std(cross_val_scores)\n",
    "\n",
    "print(f\"Mean roc_auc: {mean_roc_auc}\")\n",
    "print(f\"Standard Deviation of roc_auc: {std_roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter Tuning with Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using gen features and tdidf\n",
    "X_train_LR = hstack([X_train_gen_features,train_tdidf_csr])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'max_iter': [3000] #allow for convergence for all solvers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV 1/5] END C=0.001, max_iter=3000, penalty=l1, solver=liblinear;, score=0.544 total time=   0.2s\n",
      "[CV 2/5] END C=0.001, max_iter=3000, penalty=l1, solver=liblinear;, score=0.540 total time=   0.2s\n",
      "[CV 3/5] END C=0.001, max_iter=3000, penalty=l1, solver=liblinear;, score=0.539 total time=   0.2s\n",
      "[CV 4/5] END C=0.001, max_iter=3000, penalty=l1, solver=liblinear;, score=0.545 total time=   0.2s\n",
      "[CV 5/5] END C=0.001, max_iter=3000, penalty=l1, solver=liblinear;, score=0.541 total time=   0.2s\n",
      "[CV 1/5] END C=0.001, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.001, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.001, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.001, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.001, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.001, max_iter=3000, penalty=l2, solver=liblinear;, score=0.603 total time=   0.5s\n",
      "[CV 2/5] END C=0.001, max_iter=3000, penalty=l2, solver=liblinear;, score=0.600 total time=   0.6s\n",
      "[CV 3/5] END C=0.001, max_iter=3000, penalty=l2, solver=liblinear;, score=0.603 total time=   0.5s\n",
      "[CV 4/5] END C=0.001, max_iter=3000, penalty=l2, solver=liblinear;, score=0.604 total time=   0.6s\n",
      "[CV 5/5] END C=0.001, max_iter=3000, penalty=l2, solver=liblinear;, score=0.602 total time=   0.6s\n",
      "[CV 1/5] END C=0.001, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.607 total time=   0.3s\n",
      "[CV 2/5] END C=0.001, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.603 total time=   0.3s\n",
      "[CV 3/5] END C=0.001, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.606 total time=   0.3s\n",
      "[CV 4/5] END C=0.001, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.608 total time=   0.3s\n",
      "[CV 5/5] END C=0.001, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.605 total time=   0.3s\n",
      "[CV 1/5] END C=0.01, max_iter=3000, penalty=l1, solver=liblinear;, score=0.597 total time=   0.4s\n",
      "[CV 2/5] END C=0.01, max_iter=3000, penalty=l1, solver=liblinear;, score=0.595 total time=   0.4s\n",
      "[CV 3/5] END C=0.01, max_iter=3000, penalty=l1, solver=liblinear;, score=0.595 total time=   0.4s\n",
      "[CV 4/5] END C=0.01, max_iter=3000, penalty=l1, solver=liblinear;, score=0.601 total time=   0.4s\n",
      "[CV 5/5] END C=0.01, max_iter=3000, penalty=l1, solver=liblinear;, score=0.597 total time=   0.4s\n",
      "[CV 1/5] END C=0.01, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.01, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.01, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.01, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.01, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.01, max_iter=3000, penalty=l2, solver=liblinear;, score=0.669 total time=   0.9s\n",
      "[CV 2/5] END C=0.01, max_iter=3000, penalty=l2, solver=liblinear;, score=0.666 total time=   0.8s\n",
      "[CV 3/5] END C=0.01, max_iter=3000, penalty=l2, solver=liblinear;, score=0.669 total time=   0.8s\n",
      "[CV 4/5] END C=0.01, max_iter=3000, penalty=l2, solver=liblinear;, score=0.674 total time=   1.0s\n",
      "[CV 5/5] END C=0.01, max_iter=3000, penalty=l2, solver=liblinear;, score=0.673 total time=   0.8s\n",
      "[CV 1/5] END C=0.01, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.668 total time=   0.8s\n",
      "[CV 2/5] END C=0.01, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.666 total time=   0.7s\n",
      "[CV 3/5] END C=0.01, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.669 total time=   0.8s\n",
      "[CV 4/5] END C=0.01, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.674 total time=   4.8s\n",
      "[CV 5/5] END C=0.01, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.672 total time=   0.8s\n",
      "[CV 1/5] END C=0.1, max_iter=3000, penalty=l1, solver=liblinear;, score=0.687 total time=   0.6s\n",
      "[CV 2/5] END C=0.1, max_iter=3000, penalty=l1, solver=liblinear;, score=0.685 total time=   0.6s\n",
      "[CV 3/5] END C=0.1, max_iter=3000, penalty=l1, solver=liblinear;, score=0.692 total time=   0.6s\n",
      "[CV 4/5] END C=0.1, max_iter=3000, penalty=l1, solver=liblinear;, score=0.693 total time=   0.6s\n",
      "[CV 5/5] END C=0.1, max_iter=3000, penalty=l1, solver=liblinear;, score=0.691 total time=   0.6s\n",
      "[CV 1/5] END C=0.1, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.1, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.1, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.1, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.1, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.1, max_iter=3000, penalty=l2, solver=liblinear;, score=0.715 total time=   1.3s\n",
      "[CV 2/5] END C=0.1, max_iter=3000, penalty=l2, solver=liblinear;, score=0.715 total time=   1.4s\n",
      "[CV 3/5] END C=0.1, max_iter=3000, penalty=l2, solver=liblinear;, score=0.720 total time=   1.4s\n",
      "[CV 4/5] END C=0.1, max_iter=3000, penalty=l2, solver=liblinear;, score=0.723 total time=   1.4s\n",
      "[CV 5/5] END C=0.1, max_iter=3000, penalty=l2, solver=liblinear;, score=0.722 total time=   1.4s\n",
      "[CV 1/5] END C=0.1, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.715 total time=   3.0s\n",
      "[CV 2/5] END C=0.1, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.715 total time=   2.6s\n",
      "[CV 3/5] END C=0.1, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.720 total time=   2.8s\n",
      "[CV 4/5] END C=0.1, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.723 total time=   2.8s\n",
      "[CV 5/5] END C=0.1, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.722 total time=   2.6s\n",
      "[CV 1/5] END C=1, max_iter=3000, penalty=l1, solver=liblinear;, score=0.722 total time=   9.0s\n",
      "[CV 2/5] END C=1, max_iter=3000, penalty=l1, solver=liblinear;, score=0.721 total time=  13.6s\n",
      "[CV 3/5] END C=1, max_iter=3000, penalty=l1, solver=liblinear;, score=0.726 total time=  10.3s\n",
      "[CV 4/5] END C=1, max_iter=3000, penalty=l1, solver=liblinear;, score=0.729 total time=  16.0s\n",
      "[CV 5/5] END C=1, max_iter=3000, penalty=l1, solver=liblinear;, score=0.727 total time=  10.9s\n",
      "[CV 1/5] END C=1, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=1, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=1, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=1, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=1, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=1, max_iter=3000, penalty=l2, solver=liblinear;, score=0.718 total time=   2.3s\n",
      "[CV 2/5] END C=1, max_iter=3000, penalty=l2, solver=liblinear;, score=0.718 total time=   3.2s\n",
      "[CV 3/5] END C=1, max_iter=3000, penalty=l2, solver=liblinear;, score=0.721 total time=   3.0s\n",
      "[CV 4/5] END C=1, max_iter=3000, penalty=l2, solver=liblinear;, score=0.725 total time=   2.9s\n",
      "[CV 5/5] END C=1, max_iter=3000, penalty=l2, solver=liblinear;, score=0.724 total time=   3.1s\n",
      "[CV 1/5] END C=1, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.718 total time=   7.7s\n",
      "[CV 2/5] END C=1, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.718 total time=   8.9s\n",
      "[CV 3/5] END C=1, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.721 total time=  10.7s\n",
      "[CV 4/5] END C=1, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.725 total time=  11.5s\n",
      "[CV 5/5] END C=1, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.724 total time=   9.6s\n",
      "[CV 1/5] END C=10, max_iter=3000, penalty=l1, solver=liblinear;, score=0.692 total time=  34.2s\n",
      "[CV 2/5] END C=10, max_iter=3000, penalty=l1, solver=liblinear;, score=0.693 total time=  25.9s\n",
      "[CV 3/5] END C=10, max_iter=3000, penalty=l1, solver=liblinear;, score=0.693 total time=  28.2s\n",
      "[CV 4/5] END C=10, max_iter=3000, penalty=l1, solver=liblinear;, score=0.699 total time=  28.3s\n",
      "[CV 5/5] END C=10, max_iter=3000, penalty=l1, solver=liblinear;, score=0.700 total time=  23.8s\n",
      "[CV 1/5] END C=10, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=10, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=10, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=10, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=10, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=10, max_iter=3000, penalty=l2, solver=liblinear;, score=0.698 total time=   5.0s\n",
      "[CV 2/5] END C=10, max_iter=3000, penalty=l2, solver=liblinear;, score=0.699 total time=   5.2s\n",
      "[CV 3/5] END C=10, max_iter=3000, penalty=l2, solver=liblinear;, score=0.700 total time=   4.8s\n",
      "[CV 4/5] END C=10, max_iter=3000, penalty=l2, solver=liblinear;, score=0.705 total time=   5.9s\n",
      "[CV 5/5] END C=10, max_iter=3000, penalty=l2, solver=liblinear;, score=0.705 total time=   4.6s\n",
      "[CV 1/5] END C=10, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.699 total time=  16.5s\n",
      "[CV 2/5] END C=10, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.699 total time=  17.0s\n",
      "[CV 3/5] END C=10, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.700 total time=  18.7s\n",
      "[CV 4/5] END C=10, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.705 total time=  22.0s\n",
      "[CV 5/5] END C=10, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.705 total time=  17.7s\n",
      "[CV 1/5] END C=100, max_iter=3000, penalty=l1, solver=liblinear;, score=0.680 total time=  53.9s\n",
      "[CV 2/5] END C=100, max_iter=3000, penalty=l1, solver=liblinear;, score=0.683 total time= 1.2min\n",
      "[CV 3/5] END C=100, max_iter=3000, penalty=l1, solver=liblinear;, score=0.681 total time= 1.1min\n",
      "[CV 4/5] END C=100, max_iter=3000, penalty=l1, solver=liblinear;, score=0.688 total time=  58.5s\n",
      "[CV 5/5] END C=100, max_iter=3000, penalty=l1, solver=liblinear;, score=0.688 total time=  41.4s\n",
      "[CV 1/5] END C=100, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=100, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=100, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=100, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=100, max_iter=3000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=100, max_iter=3000, penalty=l2, solver=liblinear;, score=0.684 total time=  15.9s\n",
      "[CV 2/5] END C=100, max_iter=3000, penalty=l2, solver=liblinear;, score=0.686 total time=  23.9s\n",
      "[CV 3/5] END C=100, max_iter=3000, penalty=l2, solver=liblinear;, score=0.685 total time=  17.4s\n",
      "[CV 4/5] END C=100, max_iter=3000, penalty=l2, solver=liblinear;, score=0.691 total time=  24.9s\n",
      "[CV 5/5] END C=100, max_iter=3000, penalty=l2, solver=liblinear;, score=0.692 total time=  19.4s\n",
      "[CV 1/5] END C=100, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.684 total time=  50.3s\n",
      "[CV 2/5] END C=100, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.686 total time=  41.0s\n",
      "[CV 3/5] END C=100, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.685 total time=  59.9s\n",
      "[CV 4/5] END C=100, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.691 total time=  49.6s\n",
      "[CV 5/5] END C=100, max_iter=3000, penalty=l2, solver=lbfgs;, score=0.692 total time=  47.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "30 fits failed out of a total of 120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.54195719        nan 0.60242715 0.60599887 0.59695718        nan\n",
      " 0.67026592 0.66993911 0.6895246         nan 0.71892162 0.71891655\n",
      " 0.72493082        nan 0.72116065 0.72116092 0.69532806        nan\n",
      " 0.7015945  0.70161914 0.68392417        nan 0.68761126 0.68759062]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;max_iter&#x27;: [3000], &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;, &#x27;lbfgs&#x27;]},\n",
       "             scoring=&#x27;roc_auc&#x27;, verbose=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;max_iter&#x27;: [3000], &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;, &#x27;lbfgs&#x27;]},\n",
       "             scoring=&#x27;roc_auc&#x27;, verbose=4)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         'max_iter': [3000], 'penalty': ['l1', 'l2'],\n",
       "                         'solver': ['liblinear', 'lbfgs']},\n",
       "             scoring='roc_auc', verbose=4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='roc_auc', verbose=4)\n",
    "grid_search.fit(X_train_LR, y_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'max_iter': 3000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best Score: 0.7249308200911917\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Best score achieved during grid search\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator: LogisticRegression(C=1, max_iter=3000, penalty='l1', solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "# Best estimator (the fitted model with the best parameters)\n",
    "best_estimator = grid_search.best_estimator_\n",
    "print(\"Best Estimator:\", best_estimator)\n",
    "\n",
    "# Results for all parameter combinations\n",
    "cv_results = grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Params</th>\n",
       "      <th>Mean Score</th>\n",
       "      <th>STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C: 1, max_iter: 3000, penalty: l1, solver: liblinear</td>\n",
       "      <td>0.724931</td>\n",
       "      <td>0.003179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C: 1, max_iter: 3000, penalty: l2, solver: lbfgs</td>\n",
       "      <td>0.721161</td>\n",
       "      <td>0.002795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C: 1, max_iter: 3000, penalty: l2, solver: liblinear</td>\n",
       "      <td>0.721161</td>\n",
       "      <td>0.002796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C: 0.1, max_iter: 3000, penalty: l2, solver: liblinear</td>\n",
       "      <td>0.718922</td>\n",
       "      <td>0.003050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C: 0.1, max_iter: 3000, penalty: l2, solver: lbfgs</td>\n",
       "      <td>0.718917</td>\n",
       "      <td>0.003045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C: 10, max_iter: 3000, penalty: l2, solver: lbfgs</td>\n",
       "      <td>0.701619</td>\n",
       "      <td>0.003020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>C: 10, max_iter: 3000, penalty: l2, solver: liblinear</td>\n",
       "      <td>0.701594</td>\n",
       "      <td>0.003029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>C: 10, max_iter: 3000, penalty: l1, solver: liblinear</td>\n",
       "      <td>0.695328</td>\n",
       "      <td>0.003271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C: 0.1, max_iter: 3000, penalty: l1, solver: liblinear</td>\n",
       "      <td>0.689525</td>\n",
       "      <td>0.003116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>C: 100, max_iter: 3000, penalty: l2, solver: liblinear</td>\n",
       "      <td>0.687611</td>\n",
       "      <td>0.003421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>C: 100, max_iter: 3000, penalty: l2, solver: lbfgs</td>\n",
       "      <td>0.687591</td>\n",
       "      <td>0.003449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>C: 100, max_iter: 3000, penalty: l1, solver: liblinear</td>\n",
       "      <td>0.683924</td>\n",
       "      <td>0.003453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C: 0.01, max_iter: 3000, penalty: l2, solver: liblinear</td>\n",
       "      <td>0.670266</td>\n",
       "      <td>0.002706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C: 0.01, max_iter: 3000, penalty: l2, solver: lbfgs</td>\n",
       "      <td>0.669939</td>\n",
       "      <td>0.002728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C: 0.001, max_iter: 3000, penalty: l2, solver: lbfgs</td>\n",
       "      <td>0.605999</td>\n",
       "      <td>0.001550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C: 0.001, max_iter: 3000, penalty: l2, solver: liblinear</td>\n",
       "      <td>0.602427</td>\n",
       "      <td>0.001437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C: 0.01, max_iter: 3000, penalty: l1, solver: liblinear</td>\n",
       "      <td>0.596957</td>\n",
       "      <td>0.002181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C: 0.001, max_iter: 3000, penalty: l1, solver: liblinear</td>\n",
       "      <td>0.541957</td>\n",
       "      <td>0.002367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C: 0.001, max_iter: 3000, penalty: l1, solver: lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C: 0.01, max_iter: 3000, penalty: l1, solver: lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C: 0.1, max_iter: 3000, penalty: l1, solver: lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>C: 1, max_iter: 3000, penalty: l1, solver: lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>C: 10, max_iter: 3000, penalty: l1, solver: lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>C: 100, max_iter: 3000, penalty: l1, solver: lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Params  Mean Score  \\\n",
       "12      C: 1, max_iter: 3000, penalty: l1, solver: liblinear    0.724931   \n",
       "15          C: 1, max_iter: 3000, penalty: l2, solver: lbfgs    0.721161   \n",
       "14      C: 1, max_iter: 3000, penalty: l2, solver: liblinear    0.721161   \n",
       "10    C: 0.1, max_iter: 3000, penalty: l2, solver: liblinear    0.718922   \n",
       "11        C: 0.1, max_iter: 3000, penalty: l2, solver: lbfgs    0.718917   \n",
       "19         C: 10, max_iter: 3000, penalty: l2, solver: lbfgs    0.701619   \n",
       "18     C: 10, max_iter: 3000, penalty: l2, solver: liblinear    0.701594   \n",
       "16     C: 10, max_iter: 3000, penalty: l1, solver: liblinear    0.695328   \n",
       "8     C: 0.1, max_iter: 3000, penalty: l1, solver: liblinear    0.689525   \n",
       "22    C: 100, max_iter: 3000, penalty: l2, solver: liblinear    0.687611   \n",
       "23        C: 100, max_iter: 3000, penalty: l2, solver: lbfgs    0.687591   \n",
       "20    C: 100, max_iter: 3000, penalty: l1, solver: liblinear    0.683924   \n",
       "6    C: 0.01, max_iter: 3000, penalty: l2, solver: liblinear    0.670266   \n",
       "7        C: 0.01, max_iter: 3000, penalty: l2, solver: lbfgs    0.669939   \n",
       "3       C: 0.001, max_iter: 3000, penalty: l2, solver: lbfgs    0.605999   \n",
       "2   C: 0.001, max_iter: 3000, penalty: l2, solver: liblinear    0.602427   \n",
       "4    C: 0.01, max_iter: 3000, penalty: l1, solver: liblinear    0.596957   \n",
       "0   C: 0.001, max_iter: 3000, penalty: l1, solver: liblinear    0.541957   \n",
       "1       C: 0.001, max_iter: 3000, penalty: l1, solver: lbfgs         NaN   \n",
       "5        C: 0.01, max_iter: 3000, penalty: l1, solver: lbfgs         NaN   \n",
       "9         C: 0.1, max_iter: 3000, penalty: l1, solver: lbfgs         NaN   \n",
       "13          C: 1, max_iter: 3000, penalty: l1, solver: lbfgs         NaN   \n",
       "17         C: 10, max_iter: 3000, penalty: l1, solver: lbfgs         NaN   \n",
       "21        C: 100, max_iter: 3000, penalty: l1, solver: lbfgs         NaN   \n",
       "\n",
       "         STD  \n",
       "12  0.003179  \n",
       "15  0.002795  \n",
       "14  0.002796  \n",
       "10  0.003050  \n",
       "11  0.003045  \n",
       "19  0.003020  \n",
       "18  0.003029  \n",
       "16  0.003271  \n",
       "8   0.003116  \n",
       "22  0.003421  \n",
       "23  0.003449  \n",
       "20  0.003453  \n",
       "6   0.002706  \n",
       "7   0.002728  \n",
       "3   0.001550  \n",
       "2   0.001437  \n",
       "4   0.002181  \n",
       "0   0.002367  \n",
       "1        NaN  \n",
       "5        NaN  \n",
       "9        NaN  \n",
       "13       NaN  \n",
       "17       NaN  \n",
       "21       NaN  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_test_scores = cv_results['mean_test_score']\n",
    "std_test_scores = cv_results['std_test_score']\n",
    "params = cv_results['params']\n",
    "\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "results_df = pd.DataFrame({'Params':params,'Mean Score':mean_test_scores,'STD':std_test_scores})\n",
    "results_df[\"Params\"] = results_df[\"Params\"].apply(lambda x: ', '.join([f'{key}: {value}' for key, value in x.items()]))\n",
    "results_df.sort_values('STD', inplace=True)\n",
    "results_df.sort_values('Mean Score', ascending=False,inplace=True)\n",
    "results_df\n",
    "\n",
    "\n",
    "#for mean_score, std_score, param in zip(mean_test_scores, std_test_scores, params):\n",
    "    #print(f\"Mean Score: {mean_score:.3f} (±{std_score:.3f}) for params: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_LR = hstack([X_test_gen_features,test_tdidf_csr])\n",
    "X_test_LR = scaler.fit_transform(X_test_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc test: 0.663986533787294\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = best_estimator.predict(X_test_LR)\n",
    "roc_auc_test = roc_auc_score(y_test_LR, y_pred_test)\n",
    "print(f\"roc_auc test: {roc_auc_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
