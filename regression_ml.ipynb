{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from scipy.sparse import csr_matrix, vstack, hstack\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"train.pkl\").dropna()\n",
    "test = pd.read_pickle(\"test.pkl\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding\n",
    "encoded_train = pd.get_dummies(train, columns = ['weighted_parent_sentiment_score','weighted_comment_sentiment_score'], drop_first=True)\n",
    "encoded_test = pd.get_dummies(test, columns = ['weighted_parent_sentiment_score','weighted_comment_sentiment_score'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parent_tdidf_csr = vstack(encoded_train[\"parent_comment_tdidf\"])\n",
    "test_parent_tdidf_csr = vstack(encoded_test[\"parent_comment_tdidf\"])\n",
    "\n",
    "train_tdidf_csr = vstack(encoded_train[\"comment_tdidf\"])\n",
    "test_tdidf_csr = vstack(encoded_test[\"comment_tdidf\"])\n",
    "\n",
    "train_parent_bow_csr = vstack(encoded_train[\"parent_comment_bow\"])\n",
    "test_parent_bow_csr = vstack(encoded_test[\"parent_comment_bow\"])\n",
    "\n",
    "train_bow_csr = vstack(encoded_train[\"comment_bow\"])\n",
    "test_bow_csr = vstack(encoded_test[\"comment_bow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment', 'author', 'subreddit', 'score', 'ups', 'downs', 'date',\n",
       "       'created_utc', 'parent_comment', 'comment_tokens',\n",
       "       'parent_comment_tokens', 'comment_score', 'parent_comment_score',\n",
       "       'comment_word_count', 'parent_comment_word_count',\n",
       "       'comment_token_count', 'parent_comment_token_count',\n",
       "       'comment_unique_word_count', 'parent_comment_unique_word_count',\n",
       "       'comment_unique_token_count', 'parent_comment_unique_token_count',\n",
       "       'comment_stopword_count', 'parent_comment_stopword_count',\n",
       "       'comment_mean_word_length', 'parent_comment_mean_word_length',\n",
       "       'comment_mean_token_length', 'parent_comment_mean_token_length',\n",
       "       'comment_char_count', 'parent_comment_char_count',\n",
       "       'comment_punctuation_count', 'parent_comment_punctuation_count',\n",
       "       'comment_hashtag_count', 'parent_comment_hashtag_count',\n",
       "       'comment_number_count', 'parent_comment_number_count', 'comment_bow',\n",
       "       'parent_comment_bow', 'comment_tdidf', 'parent_comment_tdidf',\n",
       "       'comment_pos', 'parent_comment_pos', 'label',\n",
       "       'weighted_parent_sentiment_score_neutral',\n",
       "       'weighted_parent_sentiment_score_positive',\n",
       "       'weighted_comment_sentiment_score_neutral',\n",
       "       'weighted_comment_sentiment_score_positive'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_features = ['score','ups','downs',\n",
    "       'comment_word_count','parent_comment_word_count',\n",
    "       'comment_token_count', 'parent_comment_token_count',\n",
    "       'comment_unique_word_count', 'parent_comment_unique_word_count',\n",
    "       'comment_unique_token_count', 'parent_comment_unique_token_count',\n",
    "       'comment_stopword_count', 'parent_comment_stopword_count',\n",
    "       'comment_mean_word_length', 'parent_comment_mean_word_length',\n",
    "       'comment_mean_token_length', 'parent_comment_mean_token_length',\n",
    "       'comment_char_count', 'parent_comment_char_count',\n",
    "       'comment_punctuation_count', 'parent_comment_punctuation_count',\n",
    "       'comment_hashtag_count', 'parent_comment_hashtag_count',\n",
    "       'comment_number_count', 'parent_comment_number_count',\n",
    "       'weighted_parent_sentiment_score_neutral',\n",
    "       'weighted_parent_sentiment_score_positive',\n",
    "       'weighted_comment_sentiment_score_neutral',\n",
    "       'weighted_comment_sentiment_score_positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gen_features = csr_matrix(encoded_train[list_of_features])\n",
    "y_train_LR = encoded_train['label']\n",
    "\n",
    "X_test_gen_features = csr_matrix(encoded_test[list_of_features])\n",
    "y_test_LR = encoded_test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 1: General Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Specific Prep\n",
    "X_train_LR = X_train_gen_features\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.549073778456603\n",
      "Standard Deviation of Accuracy: 0.0011866425387839022\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "cross_val_scores = []\n",
    "for train_index, val_index in kf.split(X_train_LR): \n",
    "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
    "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate accuracy and store it in the list\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    cross_val_scores.append(accuracy)\n",
    "\n",
    "mean_accuracy = sum(cross_val_scores) / k\n",
    "std_accuracy = np.std(cross_val_scores)\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Standard Deviation of Accuracy: {std_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 2: Comment BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Specific Prep\n",
    "X_train_LR = train_bow_csr\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.6559869405583454\n",
      "Standard Deviation of Accuracy: 0.002837919243242117\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "cross_val_scores = []\n",
    "for train_index, val_index in kf.split(X_train_LR): \n",
    "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
    "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate accuracy and store it in the list\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    cross_val_scores.append(accuracy)\n",
    "\n",
    "mean_accuracy = sum(cross_val_scores) / k\n",
    "std_accuracy = np.std(cross_val_scores)\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Standard Deviation of Accuracy: {std_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 3: Comment TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Specific Prep\n",
    "X_train_LR = train_tdidf_csr\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.6562352661068654\n",
      "Standard Deviation of Accuracy: 0.0017712936494375434\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "cross_val_scores = []\n",
    "for train_index, val_index in kf.split(X_train_LR): \n",
    "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
    "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate accuracy and store it in the list\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    cross_val_scores.append(accuracy)\n",
    "\n",
    "mean_accuracy = sum(cross_val_scores) / k\n",
    "std_accuracy = np.std(cross_val_scores)\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Standard Deviation of Accuracy: {std_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 4: General Features + BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_LR = hstack([X_train_gen_features,train_bow_csr])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.6607484256002715\n",
      "Standard Deviation of Accuracy: 0.0023205634651901974\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "cross_val_scores = []\n",
    "for train_index, val_index in kf.split(X_train_LR): \n",
    "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
    "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate accuracy and store it in the list\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    cross_val_scores.append(accuracy)\n",
    "\n",
    "mean_accuracy = sum(cross_val_scores) / k\n",
    "std_accuracy = np.std(cross_val_scores)\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Standard Deviation of Accuracy: {std_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 5: General Features + TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_LR = hstack([X_train_gen_features,train_tdidf_csr])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.6611209176806286\n",
      "Standard Deviation of Accuracy: 0.0020015926105550713\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "cross_val_scores = []\n",
    "for train_index, val_index in kf.split(X_train_LR): \n",
    "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
    "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate accuracy and store it in the list\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    cross_val_scores.append(accuracy)\n",
    "\n",
    "mean_accuracy = sum(cross_val_scores) / k\n",
    "std_accuracy = np.std(cross_val_scores)\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Standard Deviation of Accuracy: {std_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 6: Gen Features + Comment TDIDF + Parent TDIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_LR = hstack([X_train_gen_features,train_tdidf_csr,train_parent_tdidf_csr])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.6544101625725147\n",
      "Standard Deviation of Accuracy: 0.0036873570540180743\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "cross_val_scores = []\n",
    "for train_index, val_index in kf.split(X_train_LR): \n",
    "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
    "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate accuracy and store it in the list\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    cross_val_scores.append(accuracy)\n",
    "\n",
    "mean_accuracy = sum(cross_val_scores) / k\n",
    "std_accuracy = np.std(cross_val_scores)\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Standard Deviation of Accuracy: {std_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 7: Gen + Parent TDIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_LR = hstack([X_train_gen_features,train_parent_tdidf_csr])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.568635008845028\n",
      "Standard Deviation of Accuracy: 0.0013539634505594297\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "cross_val_scores = []\n",
    "for train_index, val_index in kf.split(X_train_LR): \n",
    "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
    "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate accuracy and store it in the list\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    cross_val_scores.append(accuracy)\n",
    "\n",
    "mean_accuracy = sum(cross_val_scores) / k\n",
    "std_accuracy = np.std(cross_val_scores)\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Standard Deviation of Accuracy: {std_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter Tuning with Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using gen features and tdidf\n",
    "X_train_LR = hstack([X_train_gen_features,train_tdidf_csr])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_LR = scaler.fit_transform(X_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'max_iter': [1000] #allow for convergence for all solvers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV 1/5] END C=0.001, max_iter=1000, penalty=l1, solver=liblinear;, score=0.527 total time=   0.2s\n",
      "[CV 2/5] END C=0.001, max_iter=1000, penalty=l1, solver=liblinear;, score=0.522 total time=   0.1s\n",
      "[CV 3/5] END C=0.001, max_iter=1000, penalty=l1, solver=liblinear;, score=0.523 total time=   0.1s\n",
      "[CV 4/5] END C=0.001, max_iter=1000, penalty=l1, solver=liblinear;, score=0.526 total time=   0.1s\n",
      "[CV 5/5] END C=0.001, max_iter=1000, penalty=l1, solver=liblinear;, score=0.521 total time=   0.1s\n",
      "[CV 1/5] END C=0.001, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.001, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.001, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.001, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.001, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.001, max_iter=1000, penalty=l2, solver=liblinear;, score=0.553 total time=   0.4s\n",
      "[CV 2/5] END C=0.001, max_iter=1000, penalty=l2, solver=liblinear;, score=0.550 total time=   0.5s\n",
      "[CV 3/5] END C=0.001, max_iter=1000, penalty=l2, solver=liblinear;, score=0.550 total time=   0.4s\n",
      "[CV 4/5] END C=0.001, max_iter=1000, penalty=l2, solver=liblinear;, score=0.556 total time=   0.5s\n",
      "[CV 5/5] END C=0.001, max_iter=1000, penalty=l2, solver=liblinear;, score=0.551 total time=   0.5s\n",
      "[CV 1/5] END C=0.001, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.552 total time=   0.3s\n",
      "[CV 2/5] END C=0.001, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.549 total time=   0.3s\n",
      "[CV 3/5] END C=0.001, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.549 total time=   0.2s\n",
      "[CV 4/5] END C=0.001, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.553 total time=   0.2s\n",
      "[CV 5/5] END C=0.001, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.549 total time=   0.2s\n",
      "[CV 1/5] END C=0.01, max_iter=1000, penalty=l1, solver=liblinear;, score=0.561 total time=   0.3s\n",
      "[CV 2/5] END C=0.01, max_iter=1000, penalty=l1, solver=liblinear;, score=0.558 total time=   0.3s\n",
      "[CV 3/5] END C=0.01, max_iter=1000, penalty=l1, solver=liblinear;, score=0.559 total time=   0.3s\n",
      "[CV 4/5] END C=0.01, max_iter=1000, penalty=l1, solver=liblinear;, score=0.564 total time=   0.3s\n",
      "[CV 5/5] END C=0.01, max_iter=1000, penalty=l1, solver=liblinear;, score=0.560 total time=   0.3s\n",
      "[CV 1/5] END C=0.01, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.01, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.01, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.01, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.01, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.01, max_iter=1000, penalty=l2, solver=liblinear;, score=0.620 total time=   0.7s\n",
      "[CV 2/5] END C=0.01, max_iter=1000, penalty=l2, solver=liblinear;, score=0.619 total time=   0.7s\n",
      "[CV 3/5] END C=0.01, max_iter=1000, penalty=l2, solver=liblinear;, score=0.619 total time=   0.7s\n",
      "[CV 4/5] END C=0.01, max_iter=1000, penalty=l2, solver=liblinear;, score=0.623 total time=   0.9s\n",
      "[CV 5/5] END C=0.01, max_iter=1000, penalty=l2, solver=liblinear;, score=0.623 total time=   0.7s\n",
      "[CV 1/5] END C=0.01, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.619 total time=   0.8s\n",
      "[CV 2/5] END C=0.01, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.619 total time=   0.7s\n",
      "[CV 3/5] END C=0.01, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.618 total time=   0.7s\n",
      "[CV 4/5] END C=0.01, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.623 total time=   0.8s\n",
      "[CV 5/5] END C=0.01, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.622 total time=   0.7s\n",
      "[CV 1/5] END C=0.1, max_iter=1000, penalty=l1, solver=liblinear;, score=0.637 total time=   0.6s\n",
      "[CV 2/5] END C=0.1, max_iter=1000, penalty=l1, solver=liblinear;, score=0.635 total time=   0.6s\n",
      "[CV 3/5] END C=0.1, max_iter=1000, penalty=l1, solver=liblinear;, score=0.642 total time=   0.5s\n",
      "[CV 4/5] END C=0.1, max_iter=1000, penalty=l1, solver=liblinear;, score=0.644 total time=   0.5s\n",
      "[CV 5/5] END C=0.1, max_iter=1000, penalty=l1, solver=liblinear;, score=0.641 total time=   0.5s\n",
      "[CV 1/5] END C=0.1, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.1, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.1, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.1, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.1, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.1, max_iter=1000, penalty=l2, solver=liblinear;, score=0.661 total time=   1.2s\n",
      "[CV 2/5] END C=0.1, max_iter=1000, penalty=l2, solver=liblinear;, score=0.657 total time=   1.2s\n",
      "[CV 3/5] END C=0.1, max_iter=1000, penalty=l2, solver=liblinear;, score=0.661 total time=   1.2s\n",
      "[CV 4/5] END C=0.1, max_iter=1000, penalty=l2, solver=liblinear;, score=0.665 total time=   1.2s\n",
      "[CV 5/5] END C=0.1, max_iter=1000, penalty=l2, solver=liblinear;, score=0.662 total time=   1.2s\n",
      "[CV 1/5] END C=0.1, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.661 total time=   2.5s\n",
      "[CV 2/5] END C=0.1, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.657 total time=   2.2s\n",
      "[CV 3/5] END C=0.1, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.661 total time=   2.4s\n",
      "[CV 4/5] END C=0.1, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.665 total time=   2.4s\n",
      "[CV 5/5] END C=0.1, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.662 total time=   2.2s\n",
      "[CV 1/5] END C=1, max_iter=1000, penalty=l1, solver=liblinear;, score=0.666 total time=   8.7s\n",
      "[CV 2/5] END C=1, max_iter=1000, penalty=l1, solver=liblinear;, score=0.661 total time=   8.8s\n",
      "[CV 3/5] END C=1, max_iter=1000, penalty=l1, solver=liblinear;, score=0.667 total time=   8.9s\n",
      "[CV 4/5] END C=1, max_iter=1000, penalty=l1, solver=liblinear;, score=0.668 total time=   8.3s\n",
      "[CV 5/5] END C=1, max_iter=1000, penalty=l1, solver=liblinear;, score=0.664 total time=   8.4s\n",
      "[CV 1/5] END C=1, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=1, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=1, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=1, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=1, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=1, max_iter=1000, penalty=l2, solver=liblinear;, score=0.663 total time=   2.0s\n",
      "[CV 2/5] END C=1, max_iter=1000, penalty=l2, solver=liblinear;, score=0.659 total time=   2.6s\n",
      "[CV 3/5] END C=1, max_iter=1000, penalty=l2, solver=liblinear;, score=0.662 total time=   2.6s\n",
      "[CV 4/5] END C=1, max_iter=1000, penalty=l2, solver=liblinear;, score=0.664 total time=   2.4s\n",
      "[CV 5/5] END C=1, max_iter=1000, penalty=l2, solver=liblinear;, score=0.662 total time=   2.5s\n",
      "[CV 1/5] END C=1, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.663 total time=   6.6s\n",
      "[CV 2/5] END C=1, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.659 total time=   6.3s\n",
      "[CV 3/5] END C=1, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.662 total time=   6.4s\n",
      "[CV 4/5] END C=1, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.664 total time=   6.8s\n",
      "[CV 5/5] END C=1, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.662 total time=   6.4s\n",
      "[CV 1/5] END C=10, max_iter=1000, penalty=l1, solver=liblinear;, score=0.647 total time=  24.4s\n",
      "[CV 2/5] END C=10, max_iter=1000, penalty=l1, solver=liblinear;, score=0.644 total time=  23.9s\n",
      "[CV 3/5] END C=10, max_iter=1000, penalty=l1, solver=liblinear;, score=0.647 total time=  25.4s\n",
      "[CV 4/5] END C=10, max_iter=1000, penalty=l1, solver=liblinear;, score=0.649 total time=  24.2s\n",
      "[CV 5/5] END C=10, max_iter=1000, penalty=l1, solver=liblinear;, score=0.650 total time=  21.6s\n",
      "[CV 1/5] END C=10, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=10, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=10, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=10, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=10, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=10, max_iter=1000, penalty=l2, solver=liblinear;, score=0.648 total time=   4.7s\n",
      "[CV 2/5] END C=10, max_iter=1000, penalty=l2, solver=liblinear;, score=0.647 total time=   4.7s\n",
      "[CV 3/5] END C=10, max_iter=1000, penalty=l2, solver=liblinear;, score=0.649 total time=   4.4s\n",
      "[CV 4/5] END C=10, max_iter=1000, penalty=l2, solver=liblinear;, score=0.651 total time=   5.5s\n",
      "[CV 5/5] END C=10, max_iter=1000, penalty=l2, solver=liblinear;, score=0.651 total time=   4.7s\n",
      "[CV 1/5] END C=10, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.648 total time=  15.6s\n",
      "[CV 2/5] END C=10, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.647 total time=  16.3s\n",
      "[CV 3/5] END C=10, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.649 total time=  15.9s\n",
      "[CV 4/5] END C=10, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.651 total time=  16.0s\n",
      "[CV 5/5] END C=10, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.650 total time=  15.6s\n",
      "[CV 1/5] END C=100, max_iter=1000, penalty=l1, solver=liblinear;, score=0.642 total time=  41.5s\n",
      "[CV 2/5] END C=100, max_iter=1000, penalty=l1, solver=liblinear;, score=0.642 total time=  43.5s\n",
      "[CV 3/5] END C=100, max_iter=1000, penalty=l1, solver=liblinear;, score=0.644 total time=  42.0s\n",
      "[CV 4/5] END C=100, max_iter=1000, penalty=l1, solver=liblinear;, score=0.647 total time=  42.3s\n",
      "[CV 5/5] END C=100, max_iter=1000, penalty=l1, solver=liblinear;, score=0.646 total time=  41.0s\n",
      "[CV 1/5] END C=100, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=100, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=100, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=100, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=100, max_iter=1000, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=100, max_iter=1000, penalty=l2, solver=liblinear;, score=0.642 total time=  19.8s\n",
      "[CV 2/5] END C=100, max_iter=1000, penalty=l2, solver=liblinear;, score=0.642 total time=  29.2s\n",
      "[CV 3/5] END C=100, max_iter=1000, penalty=l2, solver=liblinear;, score=0.643 total time=  15.8s\n",
      "[CV 4/5] END C=100, max_iter=1000, penalty=l2, solver=liblinear;, score=0.646 total time=  19.7s\n",
      "[CV 5/5] END C=100, max_iter=1000, penalty=l2, solver=liblinear;, score=0.646 total time=  19.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=100, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.642 total time=  24.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=100, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.642 total time=  25.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=100, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.643 total time=  25.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=100, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.647 total time=  24.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "30 fits failed out of a total of 120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\dxcas\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.52361499        nan 0.55181147 0.55032157 0.56031635        nan\n",
      " 0.62082517 0.6204589  0.63981526        nan 0.66136302 0.6613444\n",
      " 0.66537954        nan 0.66210176 0.66211418 0.64759382        nan\n",
      " 0.64917063 0.64912096 0.64411116        nan 0.64378214 0.64394355]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=100, max_iter=1000, penalty=l2, solver=lbfgs;, score=0.647 total time=  24.6s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;max_iter&#x27;: [1000], &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;, &#x27;lbfgs&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;max_iter&#x27;: [1000], &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;, &#x27;lbfgs&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=4)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         'max_iter': [1000], 'penalty': ['l1', 'l2'],\n",
       "                         'solver': ['liblinear', 'lbfgs']},\n",
       "             scoring='accuracy', verbose=4)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy', verbose=4)\n",
    "grid_search.fit(X_train_LR, y_train_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best Score: 0.6653795446863355\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Best score achieved during grid search\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator: LogisticRegression(C=1, max_iter=1000, penalty='l1', solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "# Best estimator (the fitted model with the best parameters)\n",
    "best_estimator = grid_search.best_estimator_\n",
    "print(\"Best Estimator:\", best_estimator)\n",
    "\n",
    "# Results for all parameter combinations\n",
    "cv_results = grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Params</th>\n",
       "      <th>Mean Score</th>\n",
       "      <th>STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C: 1, max_iter: 1000, penalty: l1, solver: liblinear</td>\n",
       "      <td>0.665380</td>\n",
       "      <td>0.002526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C: 1, max_iter: 1000, penalty: l2, solver: lbfgs</td>\n",
       "      <td>0.662114</td>\n",
       "      <td>0.001566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C: 1, max_iter: 1000, penalty: l2, solver: liblinear</td>\n",
       "      <td>0.662102</td>\n",
       "      <td>0.001593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C: 0.1, max_iter: 1000, penalty: l2, solver: liblinear</td>\n",
       "      <td>0.661363</td>\n",
       "      <td>0.002492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C: 0.1, max_iter: 1000, penalty: l2, solver: lbfgs</td>\n",
       "      <td>0.661344</td>\n",
       "      <td>0.002461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>C: 10, max_iter: 1000, penalty: l2, solver: liblinear</td>\n",
       "      <td>0.649171</td>\n",
       "      <td>0.001682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C: 10, max_iter: 1000, penalty: l2, solver: lbfgs</td>\n",
       "      <td>0.649121</td>\n",
       "      <td>0.001673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>C: 10, max_iter: 1000, penalty: l1, solver: liblinear</td>\n",
       "      <td>0.647594</td>\n",
       "      <td>0.002023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>C: 100, max_iter: 1000, penalty: l1, solver: liblinear</td>\n",
       "      <td>0.644111</td>\n",
       "      <td>0.001985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>C: 100, max_iter: 1000, penalty: l2, solver: lbfgs</td>\n",
       "      <td>0.643944</td>\n",
       "      <td>0.002120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>C: 100, max_iter: 1000, penalty: l2, solver: liblinear</td>\n",
       "      <td>0.643782</td>\n",
       "      <td>0.001990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C: 0.1, max_iter: 1000, penalty: l1, solver: liblinear</td>\n",
       "      <td>0.639815</td>\n",
       "      <td>0.003339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C: 0.01, max_iter: 1000, penalty: l2, solver: liblinear</td>\n",
       "      <td>0.620825</td>\n",
       "      <td>0.001896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C: 0.01, max_iter: 1000, penalty: l2, solver: lbfgs</td>\n",
       "      <td>0.620459</td>\n",
       "      <td>0.001907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C: 0.01, max_iter: 1000, penalty: l1, solver: liblinear</td>\n",
       "      <td>0.560316</td>\n",
       "      <td>0.002156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C: 0.001, max_iter: 1000, penalty: l2, solver: liblinear</td>\n",
       "      <td>0.551811</td>\n",
       "      <td>0.002170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C: 0.001, max_iter: 1000, penalty: l2, solver: lbfgs</td>\n",
       "      <td>0.550322</td>\n",
       "      <td>0.001730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C: 0.001, max_iter: 1000, penalty: l1, solver: liblinear</td>\n",
       "      <td>0.523615</td>\n",
       "      <td>0.002123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C: 0.001, max_iter: 1000, penalty: l1, solver: lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C: 0.01, max_iter: 1000, penalty: l1, solver: lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C: 0.1, max_iter: 1000, penalty: l1, solver: lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>C: 1, max_iter: 1000, penalty: l1, solver: lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>C: 10, max_iter: 1000, penalty: l1, solver: lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>C: 100, max_iter: 1000, penalty: l1, solver: lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Params  Mean Score  \\\n",
       "12      C: 1, max_iter: 1000, penalty: l1, solver: liblinear    0.665380   \n",
       "15          C: 1, max_iter: 1000, penalty: l2, solver: lbfgs    0.662114   \n",
       "14      C: 1, max_iter: 1000, penalty: l2, solver: liblinear    0.662102   \n",
       "10    C: 0.1, max_iter: 1000, penalty: l2, solver: liblinear    0.661363   \n",
       "11        C: 0.1, max_iter: 1000, penalty: l2, solver: lbfgs    0.661344   \n",
       "18     C: 10, max_iter: 1000, penalty: l2, solver: liblinear    0.649171   \n",
       "19         C: 10, max_iter: 1000, penalty: l2, solver: lbfgs    0.649121   \n",
       "16     C: 10, max_iter: 1000, penalty: l1, solver: liblinear    0.647594   \n",
       "20    C: 100, max_iter: 1000, penalty: l1, solver: liblinear    0.644111   \n",
       "23        C: 100, max_iter: 1000, penalty: l2, solver: lbfgs    0.643944   \n",
       "22    C: 100, max_iter: 1000, penalty: l2, solver: liblinear    0.643782   \n",
       "8     C: 0.1, max_iter: 1000, penalty: l1, solver: liblinear    0.639815   \n",
       "6    C: 0.01, max_iter: 1000, penalty: l2, solver: liblinear    0.620825   \n",
       "7        C: 0.01, max_iter: 1000, penalty: l2, solver: lbfgs    0.620459   \n",
       "4    C: 0.01, max_iter: 1000, penalty: l1, solver: liblinear    0.560316   \n",
       "2   C: 0.001, max_iter: 1000, penalty: l2, solver: liblinear    0.551811   \n",
       "3       C: 0.001, max_iter: 1000, penalty: l2, solver: lbfgs    0.550322   \n",
       "0   C: 0.001, max_iter: 1000, penalty: l1, solver: liblinear    0.523615   \n",
       "1       C: 0.001, max_iter: 1000, penalty: l1, solver: lbfgs         NaN   \n",
       "5        C: 0.01, max_iter: 1000, penalty: l1, solver: lbfgs         NaN   \n",
       "9         C: 0.1, max_iter: 1000, penalty: l1, solver: lbfgs         NaN   \n",
       "13          C: 1, max_iter: 1000, penalty: l1, solver: lbfgs         NaN   \n",
       "17         C: 10, max_iter: 1000, penalty: l1, solver: lbfgs         NaN   \n",
       "21        C: 100, max_iter: 1000, penalty: l1, solver: lbfgs         NaN   \n",
       "\n",
       "         STD  \n",
       "12  0.002526  \n",
       "15  0.001566  \n",
       "14  0.001593  \n",
       "10  0.002492  \n",
       "11  0.002461  \n",
       "18  0.001682  \n",
       "19  0.001673  \n",
       "16  0.002023  \n",
       "20  0.001985  \n",
       "23  0.002120  \n",
       "22  0.001990  \n",
       "8   0.003339  \n",
       "6   0.001896  \n",
       "7   0.001907  \n",
       "4   0.002156  \n",
       "2   0.002170  \n",
       "3   0.001730  \n",
       "0   0.002123  \n",
       "1        NaN  \n",
       "5        NaN  \n",
       "9        NaN  \n",
       "13       NaN  \n",
       "17       NaN  \n",
       "21       NaN  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_test_scores = cv_results['mean_test_score']\n",
    "std_test_scores = cv_results['std_test_score']\n",
    "params = cv_results['params']\n",
    "\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "results_df = pd.DataFrame({'Params':params,'Mean Score':mean_test_scores,'STD':std_test_scores})\n",
    "results_df[\"Params\"] = results_df[\"Params\"].apply(lambda x: ', '.join([f'{key}: {value}' for key, value in x.items()]))\n",
    "results_df.sort_values('STD', inplace=True)\n",
    "results_df.sort_values('Mean Score', ascending=False,inplace=True)\n",
    "results_df\n",
    "\n",
    "\n",
    "#for mean_score, std_score, param in zip(mean_test_scores, std_test_scores, params):\n",
    "    #print(f\"Mean Score: {mean_score:.3f} (±{std_score:.3f}) for params: {param}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
