{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csr_matrix, vstack, hstack\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"data/train.pkl\").dropna()\n",
    "test = pd.read_pickle(\"data/test.pkl\").dropna()\n",
    "y_train = np.array(train['label'])\n",
    "y_test = np.array(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parent_tdidf_csr = vstack(train[\"parent_comment_tdidf\"])\n",
    "test_parent_tdidf_csr = vstack(test[\"parent_comment_tdidf\"])\n",
    "\n",
    "train_tdidf_csr = vstack(train[\"comment_tdidf_nn\"])\n",
    "test_tdidf_csr = vstack(test[\"comment_tdidf_nn\"])\n",
    "\n",
    "train_parent_bow_csr = vstack(train[\"parent_comment_bow\"])\n",
    "test_parent_bow_csr = vstack(test[\"parent_comment_bow\"])\n",
    "\n",
    "train_bow_csr = vstack(train[\"comment_bow\"])\n",
    "test_bow_csr = vstack(test[\"comment_bow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<161010x8160 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 855934 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tdidf_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_features = [\n",
    "       'comment_word_count','parent_comment_word_count',\n",
    "       'comment_token_count', 'parent_comment_token_count',\n",
    "       'comment_unique_word_count', 'parent_comment_unique_word_count',\n",
    "       'comment_unique_token_count', 'parent_comment_unique_token_count',\n",
    "       'comment_stopword_count', 'parent_comment_stopword_count',\n",
    "       'comment_mean_word_length', 'parent_comment_mean_word_length',\n",
    "       'comment_mean_token_length', 'parent_comment_mean_token_length',\n",
    "       'comment_char_count', 'parent_comment_char_count',\n",
    "       'comment_punctuation_count', 'parent_comment_punctuation_count',\n",
    "       'comment_hashtag_count', 'parent_comment_hashtag_count',\n",
    "       'comment_number_count', 'parent_comment_number_count',\n",
    "       'weighted_parent_sentiment_score_neutral',\n",
    "       'weighted_parent_sentiment_score_positive',\n",
    "       'weighted_comment_sentiment_score_neutral',\n",
    "       'weighted_comment_sentiment_score_positive']\n",
    "bool_cols = ['weighted_parent_sentiment_score_neutral',\n",
    "             'weighted_parent_sentiment_score_positive',\n",
    "             'weighted_comment_sentiment_score_neutral',\n",
    "             'weighted_comment_sentiment_score_positive']\n",
    "\n",
    "for col in bool_cols: #need to convert bool type to integer\n",
    "    train[col] = train[col].astype(int)\n",
    "    test[col] = test[col].astype(int)\n",
    "X_train_gen_features = csr_matrix(train[list_of_features])\n",
    "\n",
    "X_test_gen_features = csr_matrix(test[list_of_features])\n",
    "x_train = hstack([X_train_gen_features,train_tdidf_csr]).toarray() #deep learning automates feature selection. from our supervised learning we have learnt that BoW adds no information given tf-idf.\n",
    "x_test = hstack([X_test_gen_features,test_tdidf_csr]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18112/18112 [==============================] - 58s 3ms/step - loss: 0.6692 - accuracy: 0.5858 - val_loss: 0.6569 - val_accuracy: 0.5915\n",
      "Epoch 2/10\n",
      "18112/18112 [==============================] - 55s 3ms/step - loss: 0.6233 - accuracy: 0.6489 - val_loss: 0.6263 - val_accuracy: 0.6326\n",
      "Epoch 3/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.6036 - accuracy: 0.6683 - val_loss: 0.6144 - val_accuracy: 0.6545\n",
      "Epoch 4/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5922 - accuracy: 0.6791 - val_loss: 0.5957 - val_accuracy: 0.6740\n",
      "Epoch 5/10\n",
      "18112/18112 [==============================] - 55s 3ms/step - loss: 0.5863 - accuracy: 0.6861 - val_loss: 0.5963 - val_accuracy: 0.6743\n",
      "Epoch 6/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5803 - accuracy: 0.6921 - val_loss: 0.5942 - val_accuracy: 0.6786\n",
      "Epoch 7/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5755 - accuracy: 0.6954 - val_loss: 0.5903 - val_accuracy: 0.6812\n",
      "Epoch 8/10\n",
      "18112/18112 [==============================] - 54s 3ms/step - loss: 0.5718 - accuracy: 0.6979 - val_loss: 0.5923 - val_accuracy: 0.6787\n",
      "Epoch 9/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5696 - accuracy: 0.7002 - val_loss: 0.5957 - val_accuracy: 0.6745\n",
      "Epoch 10/10\n",
      "18112/18112 [==============================] - 55s 3ms/step - loss: 0.5664 - accuracy: 0.7031 - val_loss: 0.6007 - val_accuracy: 0.6805\n",
      "1259/1259 [==============================] - 2s 1ms/step\n",
      "AUC: 0.744722806060709\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=8, validation_split=0.1)\n",
    "y_pred_test = model.predict(x_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "print(f'AUC: {roc_auc_test}')\n",
    "#comment td-idf only AUC: 0.6628971516171749\n",
    "#gen features + comment td-idf AUC: 0.7419122793471202\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using iterative hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import HyperModel\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#use tf.keras.optimizers.legacy.Adam if on M1/M2 macbook\n",
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        # First layer\n",
    "        model.add(Dense(units=hp.Int('units_first', min_value=128, max_value=512, step=32),\n",
    "                        activation='relu', input_dim=self.input_dim))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_first', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "        \n",
    "        # Second layer\n",
    "        model.add(Dense(units=hp.Int('units_second', min_value=64, max_value=256, step=32),\n",
    "                        activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_second', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Third layer\n",
    "        model.add(Dense(units=hp.Int('units_third', min_value=32, max_value=128, step=32),\n",
    "                        activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_third', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Tuning the learning rate\n",
    "        hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "        \n",
    "        model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from keras_tuner_dir\\keras_tuner\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cmz\\AppData\\Local\\Temp\\ipykernel_19772\\215952752.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import Hyperband\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "hypermodel = MyHyperModel(input_dim=x_train.shape[1])\n",
    "\n",
    "tuner = Hyperband(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='keras_tuner'\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(x_train, y_train,\n",
    "             epochs=50,\n",
    "             validation_split=0.1,\n",
    "             callbacks=[stop_early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 352)               2860000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 352)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 160)               56480     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 160)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                10304     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2926849 (11.17 MB)\n",
      "Trainable params: 2926849 (11.17 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "18112/18112 [==============================] - 288s 16ms/step - loss: 0.6918 - accuracy: 0.5392 - val_loss: 0.6800 - val_accuracy: 0.5556\n",
      "Epoch 2/10\n",
      "18112/18112 [==============================] - 284s 16ms/step - loss: 0.6771 - accuracy: 0.5738 - val_loss: 0.6659 - val_accuracy: 0.5992\n",
      "Epoch 3/10\n",
      "18112/18112 [==============================] - 284s 16ms/step - loss: 0.6541 - accuracy: 0.6188 - val_loss: 0.6265 - val_accuracy: 0.6534\n",
      "Epoch 4/10\n",
      "18112/18112 [==============================] - 282s 16ms/step - loss: 0.6308 - accuracy: 0.6509 - val_loss: 0.6173 - val_accuracy: 0.6649\n",
      "Epoch 5/10\n",
      "18112/18112 [==============================] - 285s 16ms/step - loss: 0.6232 - accuracy: 0.6596 - val_loss: 0.6367 - val_accuracy: 0.6330\n",
      "Epoch 6/10\n",
      "18112/18112 [==============================] - 285s 16ms/step - loss: 0.6126 - accuracy: 0.6684 - val_loss: 0.6020 - val_accuracy: 0.6760\n",
      "Epoch 7/10\n",
      "18112/18112 [==============================] - 283s 16ms/step - loss: 0.6059 - accuracy: 0.6722 - val_loss: 0.6136 - val_accuracy: 0.6528\n",
      "Epoch 8/10\n",
      "18112/18112 [==============================] - 284s 16ms/step - loss: 0.6031 - accuracy: 0.6769 - val_loss: 0.6377 - val_accuracy: 0.6359\n",
      "Epoch 9/10\n",
      "18112/18112 [==============================] - 295s 16ms/step - loss: 0.5994 - accuracy: 0.6811 - val_loss: 0.6392 - val_accuracy: 0.6636\n",
      "Epoch 10/10\n",
      "18112/18112 [==============================] - 295s 16ms/step - loss: 0.5962 - accuracy: 0.6824 - val_loss: 0.6033 - val_accuracy: 0.6695\n",
      "1259/1259 [==============================] - 4s 3ms/step\n",
      "Best model AUC: 0.7365322174788665\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "# Summary of the best model\n",
    "best_model.summary()\n",
    "# Optionally, you can retrain the model with the best hyperparameters on the full dataset\n",
    "best_model.fit(x_train, y_train, epochs=10, batch_size=8, validation_split=0.1)\n",
    "y_pred_test = best_model.predict(x_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "print(f'Best model AUC: {roc_auc_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN implementation\n",
    "\n",
    "# Idea: \n",
    "- 2 LSTM branches-> iteratively takes in comment tokens, and parent comment tokens\n",
    "- Dense branch-> dense layer takes in the other features\n",
    "- Merge branches\n",
    "- one more Dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_features = [\n",
    "       'comment_word_count','parent_comment_word_count',\n",
    "       'comment_token_count', 'parent_comment_token_count',\n",
    "       'comment_unique_word_count', 'parent_comment_unique_word_count',\n",
    "       'comment_unique_token_count', 'parent_comment_unique_token_count',\n",
    "       'comment_stopword_count', 'parent_comment_stopword_count',\n",
    "       'comment_mean_word_length', 'parent_comment_mean_word_length',\n",
    "       'comment_mean_token_length', 'parent_comment_mean_token_length',\n",
    "       'comment_char_count', 'parent_comment_char_count',\n",
    "       'comment_punctuation_count', 'parent_comment_punctuation_count',\n",
    "       'comment_hashtag_count', 'parent_comment_hashtag_count',\n",
    "       'comment_number_count', 'parent_comment_number_count',\n",
    "       'weighted_parent_sentiment_score_neutral',\n",
    "       'weighted_parent_sentiment_score_positive',\n",
    "       'weighted_comment_sentiment_score_neutral',\n",
    "       'weighted_comment_sentiment_score_positive', 'documents_comment']#, 'documents_parent_comment'] #if doesnt work we try comment_tdidf_nn\n",
    "train[\"documents_comment\"] = train['comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list)) #keras needs to use own tokenizer\n",
    "#train[\"documents_parent_comment\"] = train['parent_comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list))\n",
    "test[\"documents_comment\"] = test['comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list))\n",
    "#test[\"documents_parent_comment\"] = test['parent_comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list))\n",
    "\n",
    "rnn_train = train[rnn_features]\n",
    "rnn_test = test[rnn_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, TextVectorization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "# TextVectorization for comment and parent_comment\n",
    "max_features = 8000 #follow number of tokens for feedforward\n",
    "\n",
    "\n",
    "vectorize_layer_comment = TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    split='whitespace',\n",
    "    ngrams=3\n",
    "    )\n",
    "# Prepare dataset for TextVectorization adapt\n",
    "train_texts = rnn_train['documents_comment'].tolist()\n",
    "test_texts = rnn_test['documents_comment'].tolist()\n",
    "vectorize_layer_comment.adapt(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "#use tf.keras.optimizers.legacy.Adam if on M1/M2 macbook\n",
    "# Assuming max_features is the vocabulary size and embedding_dim is the dimension of the embedding\n",
    "embedding_dim = 128  # You can choose an appropriate value\n",
    "\n",
    "# Add an Embedding layer after text vectorization\n",
    "embedding_layer = Embedding(max_features, embedding_dim)\n",
    "\n",
    "# LSTM Branch\n",
    "text_input_comment = Input(shape=(1,), dtype=tf.string, name='text_comment')\n",
    "text_features_comment = vectorize_layer_comment(text_input_comment)\n",
    "text_features_comment = embedding_layer(text_features_comment)  # Embedding layer\n",
    "lstm_comment = LSTM(64)(text_features_comment)\n",
    "\n",
    "# Dense Features Branch\n",
    "other_features_input = Input(shape=(len(rnn_features) - 1,), name='other_features')\n",
    "dense_features = Dense(128, activation='relu')(other_features_input)\n",
    "\n",
    "# Concatenate\n",
    "concatenated = Concatenate()([lstm_comment, dense_features])\n",
    "\n",
    "# Additional Dense Layers\n",
    "output = Dense(64, activation='relu')(concatenated)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "# Build Model\n",
    "model = Model(inputs=[text_input_comment, other_features_input], outputs=output)\n",
    "optimiser = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "# Compile\n",
    "model.compile(optimizer=optimiser, loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2013/2013 [==============================] - 92s 45ms/step - loss: 0.8198 - accuracy: 0.5357 - val_loss: 0.7716 - val_accuracy: 0.5347\n",
      "Epoch 2/30\n",
      "2013/2013 [==============================] - 94s 47ms/step - loss: 0.7003 - accuracy: 0.5567 - val_loss: 0.6788 - val_accuracy: 0.5687\n",
      "Epoch 3/30\n",
      "2013/2013 [==============================] - 94s 47ms/step - loss: 0.6904 - accuracy: 0.5601 - val_loss: 0.6802 - val_accuracy: 0.5639\n",
      "Epoch 4/30\n",
      "2013/2013 [==============================] - 93s 46ms/step - loss: 0.6860 - accuracy: 0.5623 - val_loss: 0.6762 - val_accuracy: 0.5691\n",
      "Epoch 5/30\n",
      "2013/2013 [==============================] - 86s 43ms/step - loss: 0.6773 - accuracy: 0.5677 - val_loss: 0.6784 - val_accuracy: 0.5624\n",
      "Epoch 6/30\n",
      "2013/2013 [==============================] - 87s 43ms/step - loss: 0.6770 - accuracy: 0.5690 - val_loss: 0.6741 - val_accuracy: 0.5715\n",
      "Epoch 7/30\n",
      "2013/2013 [==============================] - 94s 47ms/step - loss: 0.6745 - accuracy: 0.5719 - val_loss: 0.6761 - val_accuracy: 0.5647\n",
      "Epoch 8/30\n",
      "2013/2013 [==============================] - 92s 46ms/step - loss: 0.6756 - accuracy: 0.5723 - val_loss: 0.6745 - val_accuracy: 0.5748\n",
      "Epoch 9/30\n",
      " 441/2013 [=====>........................] - ETA: 1:19 - loss: 0.6732 - accuracy: 0.5747"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m test_other_features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(rnn_test[feature_columns])\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Fit the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     [train_texts, train_other_features], y_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m  \u001b[39m# or use a validation set\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m test_loss, test_accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate([test_texts, test_other_features], y_test)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[1;32m    869\u001b[0m   )\n\u001b[1;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    253\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    254\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    255\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1485\u001b[0m   )\n\u001b[1;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare text inputs\n",
    "train_texts = np.array(train_texts)[:, np.newaxis]\n",
    "test_texts = np.array(test_texts)[:, np.newaxis]\n",
    "\n",
    "# Prepare other features inputs\n",
    "feature_columns = [col for col in rnn_features if col != 'documents_comment']\n",
    "\n",
    "train_other_features = np.array(rnn_train[feature_columns]).astype(np.float32)\n",
    "test_other_features = np.array(rnn_test[feature_columns]).astype(np.float32)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    [train_texts, train_other_features], y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_split=0.25  # or use a validation set\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate([test_texts, test_other_features], y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_probs = model.predict([test_texts, test_other_features]).ravel()\n",
    "\n",
    "# Calculate ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"ROC-AUC Score: {roc_auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
