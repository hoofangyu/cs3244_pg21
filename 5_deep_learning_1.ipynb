{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #pandas 1.5.3\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csr_matrix, vstack, hstack\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from joblib import load\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from lime.lime_tabular import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.0 when using version 1.3.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.0 when using version 1.3.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle(\"data/train.pkl\")\n",
    "test = pd.read_pickle(\"data/test.pkl\")\n",
    "tfidf_vec = load('tdvectorizer_nn.pkl')\n",
    "y_train = np.array(train['label'])\n",
    "y_test = np.array(test['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (241496, 11639), indices imply (241496, 6000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m test_tfidf_csr \u001b[39m=\u001b[39m vstack(test[\u001b[39m\"\u001b[39m\u001b[39mcomment_tfidf_nn\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Convert the TF-IDF vectors to a DataFrame\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_tfidf_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(train_tfidf_csr\u001b[39m.\u001b[39;49mtoarray(), columns\u001b[39m=\u001b[39;49mfeature_names)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m test_tfidf_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(test_tfidf_csr\u001b[39m.\u001b[39mtoarray(), columns\u001b[39m=\u001b[39mfeature_names) \u001b[39m# Now train_tfidf_df and test_tfidf_df have the original feature names as columns\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:722\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    712\u001b[0m         mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    713\u001b[0m             \u001b[39m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    714\u001b[0m             \u001b[39m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    719\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    720\u001b[0m         )\n\u001b[1;32m    721\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[1;32m    723\u001b[0m             data,\n\u001b[1;32m    724\u001b[0m             index,\n\u001b[1;32m    725\u001b[0m             columns,\n\u001b[1;32m    726\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    727\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    728\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[1;32m    729\u001b[0m         )\n\u001b[1;32m    731\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/internals/construction.py:349\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    345\u001b[0m index, columns \u001b[39m=\u001b[39m _get_axes(\n\u001b[1;32m    346\u001b[0m     values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns\n\u001b[1;32m    347\u001b[0m )\n\u001b[0;32m--> 349\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/internals/construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m passed \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[1;32m    419\u001b[0m implied \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(index), \u001b[39mlen\u001b[39m(columns))\n\u001b[0;32m--> 420\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of passed values is \u001b[39m\u001b[39m{\u001b[39;00mpassed\u001b[39m}\u001b[39;00m\u001b[39m, indices imply \u001b[39m\u001b[39m{\u001b[39;00mimplied\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (241496, 11639), indices imply (241496, 6000)"
     ]
    }
   ],
   "source": [
    "# Assuming tfidf_vec is your TF-IDF vectorizer and train_tfidf_features is your TF-IDF sparse matrix\n",
    "feature_names = tfidf_vec.get_feature_names_out()\n",
    "\n",
    "train_tfidf_csr = vstack(train[\"comment_tfidf_nn\"])\n",
    "test_tfidf_csr = vstack(test[\"comment_tfidf_nn\"])\n",
    "# Convert the TF-IDF vectors to a DataFrame\n",
    "train_tfidf_df = pd.DataFrame(train_tfidf_csr.toarray(), columns=feature_names)\n",
    "test_tfidf_df = pd.DataFrame(test_tfidf_csr.toarray(), columns=feature_names) # Now train_tfidf_df and test_tfidf_df have the original feature names as columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_features = [\n",
    "       'comment_word_count','parent_comment_word_count',\n",
    "       'comment_token_count', 'parent_comment_token_count',\n",
    "       'comment_unique_word_count', 'parent_comment_unique_word_count',\n",
    "       'comment_unique_token_count', 'parent_comment_unique_token_count',\n",
    "       'comment_stopword_count', 'parent_comment_stopword_count',\n",
    "       'comment_mean_word_length', 'parent_comment_mean_word_length',\n",
    "       'comment_mean_token_length', 'parent_comment_mean_token_length',\n",
    "       'comment_char_count', 'parent_comment_char_count',\n",
    "       'comment_punctuation_count', 'parent_comment_punctuation_count',\n",
    "       'comment_hashtag_count', 'parent_comment_hashtag_count',\n",
    "       'comment_number_count', 'parent_comment_number_count',\n",
    "       'weighted_parent_sentiment_score_neutral',\n",
    "       'weighted_parent_sentiment_score_positive',\n",
    "       'weighted_comment_sentiment_score_neutral',\n",
    "       'weighted_comment_sentiment_score_positive']\n",
    "bool_cols = ['weighted_parent_sentiment_score_neutral',\n",
    "             'weighted_parent_sentiment_score_positive',\n",
    "             'weighted_comment_sentiment_score_neutral',\n",
    "             'weighted_comment_sentiment_score_positive']\n",
    "\n",
    "for col in bool_cols: #need to convert bool type to integer\n",
    "    train[col] = train[col].astype(int)\n",
    "    test[col] = test[col].astype(int)\n",
    "\n",
    "x_train = pd.concat([train[list_of_features].reset_index(drop=True), train_tfidf_df.reset_index(drop=True)], axis=1)\n",
    "x_test = pd.concat([test[list_of_features].reset_index(drop=True), test_tfidf_df.reset_index(drop=True)], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward NN\n",
    "- We have tuned a baseline model using keras tuner, of which the code is below. We managed to achieve about 0.744 AUC at best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=8, validation_split=0.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising ROC AUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(x_test)\n",
    "#roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "#comment td-idf only AUC: 0.6628971516171749\n",
    "#gen features + comment td-idf AUC: 0.7419122793471202\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_auc_test = auc(fpr, tpr)\n",
    "print(f'AUC: {roc_auc_test}')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='No Skill')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.hstack([raw_text_data, raw_other_features])\n",
    "feature_names = ['comment_text'] + list_of_features  # Assuming 'comment_text' is the name of your text column\n",
    "explainer = LimeTabularExplainer(\n",
    "    training_data=raw_data,\n",
    "    feature_names=feature_names,\n",
    "    class_names=['class_0', 'class_1'],  # Update with your class names\n",
    "    discretize_continuous=True\n",
    ")\n",
    "def predict_fn(data):\n",
    "    # Split the data back into text and other features\n",
    "    text_data = data[:, 0]\n",
    "    other_features_data = data[:, 1:]\n",
    "\n",
    "    # Preprocess the text data and other features\n",
    "    # For text, use your tfidf_vec object\n",
    "    text_data_tfidf = tfidf_vec.transform(text_data)\n",
    "\n",
    "    # For other features, apply any required preprocessing (e.g., scaling)\n",
    "    # ...\n",
    "\n",
    "    # Combine them back\n",
    "    final_data = hstack([text_data_tfidf, other_features_data]).toarray()\n",
    "\n",
    "    # Get predictions\n",
    "    preds = model.predict(final_data)\n",
    "    return preds\n",
    "idx_to_explain = 50  # Example index\n",
    "data_to_explain = raw_data[idx_to_explain]\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=data_to_explain,\n",
    "    predict_fn=predict_fn\n",
    ")\n",
    "\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using iterative hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "from keras_tuner import HyperModel\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#use tf.keras.optimizers.legacy.Adam if on M1/M2 macbook\n",
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        # First layer\n",
    "        model.add(Dense(units=hp.Int('units_first', min_value=128, max_value=512, step=32),\n",
    "                        activation='relu', input_dim=self.input_dim))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_first', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "        \n",
    "        # Second layer\n",
    "        model.add(Dense(units=hp.Int('units_second', min_value=64, max_value=256, step=32),\n",
    "                        activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_second', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Third layer\n",
    "        model.add(Dense(units=hp.Int('units_third', min_value=32, max_value=128, step=32),\n",
    "                        activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_third', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Tuning the learning rate\n",
    "        hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "        \n",
    "        model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from keras_tuner_dir\\keras_tuner\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cmz\\AppData\\Local\\Temp\\ipykernel_19772\\215952752.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import Hyperband\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "hypermodel = MyHyperModel(input_dim=x_train.shape[1])\n",
    "\n",
    "tuner = Hyperband(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='keras_tuner'\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(x_train, y_train,\n",
    "             epochs=50,\n",
    "             validation_split=0.1,\n",
    "             callbacks=[stop_early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 352)               2860000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 352)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 160)               56480     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 160)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                10304     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2926849 (11.17 MB)\n",
      "Trainable params: 2926849 (11.17 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "18112/18112 [==============================] - 288s 16ms/step - loss: 0.6918 - accuracy: 0.5392 - val_loss: 0.6800 - val_accuracy: 0.5556\n",
      "Epoch 2/10\n",
      "18112/18112 [==============================] - 284s 16ms/step - loss: 0.6771 - accuracy: 0.5738 - val_loss: 0.6659 - val_accuracy: 0.5992\n",
      "Epoch 3/10\n",
      "18112/18112 [==============================] - 284s 16ms/step - loss: 0.6541 - accuracy: 0.6188 - val_loss: 0.6265 - val_accuracy: 0.6534\n",
      "Epoch 4/10\n",
      "18112/18112 [==============================] - 282s 16ms/step - loss: 0.6308 - accuracy: 0.6509 - val_loss: 0.6173 - val_accuracy: 0.6649\n",
      "Epoch 5/10\n",
      "18112/18112 [==============================] - 285s 16ms/step - loss: 0.6232 - accuracy: 0.6596 - val_loss: 0.6367 - val_accuracy: 0.6330\n",
      "Epoch 6/10\n",
      "18112/18112 [==============================] - 285s 16ms/step - loss: 0.6126 - accuracy: 0.6684 - val_loss: 0.6020 - val_accuracy: 0.6760\n",
      "Epoch 7/10\n",
      "18112/18112 [==============================] - 283s 16ms/step - loss: 0.6059 - accuracy: 0.6722 - val_loss: 0.6136 - val_accuracy: 0.6528\n",
      "Epoch 8/10\n",
      "18112/18112 [==============================] - 284s 16ms/step - loss: 0.6031 - accuracy: 0.6769 - val_loss: 0.6377 - val_accuracy: 0.6359\n",
      "Epoch 9/10\n",
      "18112/18112 [==============================] - 295s 16ms/step - loss: 0.5994 - accuracy: 0.6811 - val_loss: 0.6392 - val_accuracy: 0.6636\n",
      "Epoch 10/10\n",
      "18112/18112 [==============================] - 295s 16ms/step - loss: 0.5962 - accuracy: 0.6824 - val_loss: 0.6033 - val_accuracy: 0.6695\n",
      "1259/1259 [==============================] - 4s 3ms/step\n",
      "Best model AUC: 0.7365322174788665\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "# Summary of the best model\n",
    "best_model.summary()\n",
    "# Optionally, you can retrain the model with the best hyperparameters on the full dataset\n",
    "#best_model.fit(x_train, y_train, epochs=10, batch_size=8, validation_split=0.1)\n",
    "#y_pred_test = best_model.predict(x_test)\n",
    "#roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "#print(f'Best model AUC: {roc_auc_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN implementation\n",
    "\n",
    "# Idea: \n",
    "- LSTM branch-> iteratively takes in comment tokens\n",
    "- Dense branch-> dense layer takes in the other features\n",
    "- Merge branches\n",
    "- one more Dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_features = [\n",
    "       'comment_word_count','parent_comment_word_count',\n",
    "       'comment_token_count', 'parent_comment_token_count',\n",
    "       'comment_unique_word_count', 'parent_comment_unique_word_count',\n",
    "       'comment_unique_token_count', 'parent_comment_unique_token_count',\n",
    "       'comment_stopword_count', 'parent_comment_stopword_count',\n",
    "       'comment_mean_word_length', 'parent_comment_mean_word_length',\n",
    "       'comment_mean_token_length', 'parent_comment_mean_token_length',\n",
    "       'comment_char_count', 'parent_comment_char_count',\n",
    "       'comment_punctuation_count', 'parent_comment_punctuation_count',\n",
    "       'comment_hashtag_count', 'parent_comment_hashtag_count',\n",
    "       'comment_number_count', 'parent_comment_number_count',\n",
    "       'weighted_parent_sentiment_score_neutral',\n",
    "       'weighted_parent_sentiment_score_positive',\n",
    "       'weighted_comment_sentiment_score_neutral',\n",
    "       'weighted_comment_sentiment_score_positive', 'documents_comment']#, 'documents_parent_comment'] #if doesnt work we try comment_tfidf_nn\n",
    "train[\"documents_comment\"] = train['comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list)) #keras needs to use own tokenizer\n",
    "#train[\"documents_parent_comment\"] = train['parent_comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list))\n",
    "test[\"documents_comment\"] = test['comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list))\n",
    "#test[\"documents_parent_comment\"] = test['parent_comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list))\n",
    "\n",
    "rnn_train = train[rnn_features]\n",
    "rnn_test = test[rnn_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, TextVectorization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "# TextVectorization for comment and parent_comment\n",
    "max_features = 8000 #follow number of tokens for feedforward\n",
    "\n",
    "\n",
    "vectorize_layer_comment = TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    split='whitespace',\n",
    "    ngrams=3\n",
    "    )\n",
    "# Prepare dataset for TextVectorization adapt\n",
    "train_texts = rnn_train['documents_comment'].tolist()\n",
    "test_texts = rnn_test['documents_comment'].tolist()\n",
    "vectorize_layer_comment.adapt(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "#use tf.keras.optimizers.legacy.Adam if on M1/M2 macbook\n",
    "# Assuming max_features is the vocabulary size and embedding_dim is the dimension of the embedding\n",
    "embedding_dim = 128  # You can choose an appropriate value\n",
    "\n",
    "# Add an Embedding layer after text vectorization\n",
    "embedding_layer = Embedding(max_features, embedding_dim)\n",
    "\n",
    "# LSTM Branch\n",
    "text_input_comment = Input(shape=(1,), dtype=tf.string, name='text_comment')\n",
    "text_features_comment = vectorize_layer_comment(text_input_comment)\n",
    "text_features_comment = embedding_layer(text_features_comment)  # Embedding layer\n",
    "lstm_comment = LSTM(64)(text_features_comment)\n",
    "\n",
    "# Dense Features Branch\n",
    "other_features_input = Input(shape=(len(rnn_features) - 1,), name='other_features')\n",
    "dense_features = Dense(128, activation='relu')(other_features_input)\n",
    "\n",
    "# Concatenate\n",
    "concatenated = Concatenate()([lstm_comment, dense_features])\n",
    "\n",
    "# Additional Dense Layers\n",
    "output = Dense(64, activation='relu')(concatenated)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "# Build Model\n",
    "model = Model(inputs=[text_input_comment, other_features_input], outputs=output)\n",
    "#optimiser = tf.keras.optimizers.legacy.SGD(learning_rate=0.001)\n",
    "# Compile\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])#use Adam optimiser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare text inputs\n",
    "train_texts = np.array(train_texts)[:, np.newaxis]\n",
    "test_texts = np.array(test_texts)[:, np.newaxis]\n",
    "\n",
    "# Prepare other features inputs\n",
    "feature_columns = [col for col in rnn_features if col != 'documents_comment']\n",
    "\n",
    "train_other_features = np.array(rnn_train[feature_columns]).astype(np.float32)\n",
    "test_other_features = np.array(rnn_test[feature_columns]).astype(np.float32)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    [train_texts, train_other_features], y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1  # or use a validation set\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate([test_texts, test_other_features], y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_probs = model.predict([test_texts, test_other_features]).ravel()\n",
    "\n",
    "# Calculate ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"ROC-AUC Score: {roc_auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
