{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #pandas 1.5.3\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csr_matrix, vstack, hstack\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.0 when using version 1.3.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.0 when using version 1.3.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle(\"data/train.pkl\")\n",
    "test = pd.read_pickle(\"data/test.pkl\")\n",
    "tfidf_vec = load('tdvectorizer_nn.pkl')\n",
    "y_train = np.array(train['label'])\n",
    "y_test = np.array(test['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parent_tfidf_csr = vstack(train[\"parent_comment_tfidf\"])\n",
    "test_parent_tfidf_csr = vstack(test[\"parent_comment_tfidf\"])\n",
    "\n",
    "train_tfidf_csr = vstack(train[\"comment_tfidf_nn\"])\n",
    "test_tfidf_csr = vstack(test[\"comment_tfidf_nn\"])\n",
    "\n",
    "train_parent_bow_csr = vstack(train[\"parent_comment_bow\"])\n",
    "test_parent_bow_csr = vstack(test[\"parent_comment_bow\"])\n",
    "\n",
    "train_bow_csr = vstack(train[\"comment_bow\"])\n",
    "test_bow_csr = vstack(test[\"comment_bow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<241496x11639 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1341091 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_features = [\n",
    "       'comment_word_count','parent_comment_word_count',\n",
    "       'comment_token_count', 'parent_comment_token_count',\n",
    "       'comment_unique_word_count', 'parent_comment_unique_word_count',\n",
    "       'comment_unique_token_count', 'parent_comment_unique_token_count',\n",
    "       'comment_stopword_count', 'parent_comment_stopword_count',\n",
    "       'comment_mean_word_length', 'parent_comment_mean_word_length',\n",
    "       'comment_mean_token_length', 'parent_comment_mean_token_length',\n",
    "       'comment_char_count', 'parent_comment_char_count',\n",
    "       'comment_punctuation_count', 'parent_comment_punctuation_count',\n",
    "       'comment_hashtag_count', 'parent_comment_hashtag_count',\n",
    "       'comment_number_count', 'parent_comment_number_count',\n",
    "       'weighted_parent_sentiment_score_neutral',\n",
    "       'weighted_parent_sentiment_score_positive',\n",
    "       'weighted_comment_sentiment_score_neutral',\n",
    "       'weighted_comment_sentiment_score_positive']\n",
    "bool_cols = ['weighted_parent_sentiment_score_neutral',\n",
    "             'weighted_parent_sentiment_score_positive',\n",
    "             'weighted_comment_sentiment_score_neutral',\n",
    "             'weighted_comment_sentiment_score_positive']\n",
    "\n",
    "for col in bool_cols: #need to convert bool type to integer\n",
    "    train[col] = train[col].astype(int)\n",
    "    test[col] = test[col].astype(int)\n",
    "X_train_gen_features = csr_matrix(train[list_of_features])\n",
    "\n",
    "X_test_gen_features = csr_matrix(test[list_of_features])\n",
    "x_train = hstack([X_train_gen_features,train_tfidf_csr]).toarray() #deep learning automates feature selection. from our supervised learning we have learnt that BoW adds no information given tf-idf.\n",
    "x_test = hstack([X_test_gen_features,test_tfidf_csr]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18112/18112 [==============================] - 58s 3ms/step - loss: 0.6692 - accuracy: 0.5858 - val_loss: 0.6569 - val_accuracy: 0.5915\n",
      "Epoch 2/10\n",
      "18112/18112 [==============================] - 55s 3ms/step - loss: 0.6233 - accuracy: 0.6489 - val_loss: 0.6263 - val_accuracy: 0.6326\n",
      "Epoch 3/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.6036 - accuracy: 0.6683 - val_loss: 0.6144 - val_accuracy: 0.6545\n",
      "Epoch 4/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5922 - accuracy: 0.6791 - val_loss: 0.5957 - val_accuracy: 0.6740\n",
      "Epoch 5/10\n",
      "18112/18112 [==============================] - 55s 3ms/step - loss: 0.5863 - accuracy: 0.6861 - val_loss: 0.5963 - val_accuracy: 0.6743\n",
      "Epoch 6/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5803 - accuracy: 0.6921 - val_loss: 0.5942 - val_accuracy: 0.6786\n",
      "Epoch 7/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5755 - accuracy: 0.6954 - val_loss: 0.5903 - val_accuracy: 0.6812\n",
      "Epoch 8/10\n",
      "18112/18112 [==============================] - 54s 3ms/step - loss: 0.5718 - accuracy: 0.6979 - val_loss: 0.5923 - val_accuracy: 0.6787\n",
      "Epoch 9/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5696 - accuracy: 0.7002 - val_loss: 0.5957 - val_accuracy: 0.6745\n",
      "Epoch 10/10\n",
      "18112/18112 [==============================] - 55s 3ms/step - loss: 0.5664 - accuracy: 0.7031 - val_loss: 0.6007 - val_accuracy: 0.6805\n",
      "1259/1259 [==============================] - 2s 1ms/step\n",
      "AUC: 0.744722806060709\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=8, validation_split=0.1)\n",
    "y_pred_test = model.predict(x_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "print(f'AUC: {roc_auc_test}')\n",
    "#comment td-idf only AUC: 0.6628971516171749\n",
    "#gen features + comment td-idf AUC: 0.7419122793471202\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using iterative hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "from keras_tuner import HyperModel\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#use tf.keras.optimizers.legacy.Adam if on M1/M2 macbook\n",
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        # First layer\n",
    "        model.add(Dense(units=hp.Int('units_first', min_value=128, max_value=512, step=32),\n",
    "                        activation='relu', input_dim=self.input_dim))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_first', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "        \n",
    "        # Second layer\n",
    "        model.add(Dense(units=hp.Int('units_second', min_value=64, max_value=256, step=32),\n",
    "                        activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_second', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Third layer\n",
    "        model.add(Dense(units=hp.Int('units_third', min_value=32, max_value=128, step=32),\n",
    "                        activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_third', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Tuning the learning rate\n",
    "        hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "        \n",
    "        model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from keras_tuner_dir\\keras_tuner\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cmz\\AppData\\Local\\Temp\\ipykernel_19772\\215952752.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import Hyperband\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "hypermodel = MyHyperModel(input_dim=x_train.shape[1])\n",
    "\n",
    "tuner = Hyperband(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='keras_tuner'\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(x_train, y_train,\n",
    "             epochs=50,\n",
    "             validation_split=0.1,\n",
    "             callbacks=[stop_early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 352)               2860000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 352)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 160)               56480     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 160)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                10304     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2926849 (11.17 MB)\n",
      "Trainable params: 2926849 (11.17 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "18112/18112 [==============================] - 288s 16ms/step - loss: 0.6918 - accuracy: 0.5392 - val_loss: 0.6800 - val_accuracy: 0.5556\n",
      "Epoch 2/10\n",
      "18112/18112 [==============================] - 284s 16ms/step - loss: 0.6771 - accuracy: 0.5738 - val_loss: 0.6659 - val_accuracy: 0.5992\n",
      "Epoch 3/10\n",
      "18112/18112 [==============================] - 284s 16ms/step - loss: 0.6541 - accuracy: 0.6188 - val_loss: 0.6265 - val_accuracy: 0.6534\n",
      "Epoch 4/10\n",
      "18112/18112 [==============================] - 282s 16ms/step - loss: 0.6308 - accuracy: 0.6509 - val_loss: 0.6173 - val_accuracy: 0.6649\n",
      "Epoch 5/10\n",
      "18112/18112 [==============================] - 285s 16ms/step - loss: 0.6232 - accuracy: 0.6596 - val_loss: 0.6367 - val_accuracy: 0.6330\n",
      "Epoch 6/10\n",
      "18112/18112 [==============================] - 285s 16ms/step - loss: 0.6126 - accuracy: 0.6684 - val_loss: 0.6020 - val_accuracy: 0.6760\n",
      "Epoch 7/10\n",
      "18112/18112 [==============================] - 283s 16ms/step - loss: 0.6059 - accuracy: 0.6722 - val_loss: 0.6136 - val_accuracy: 0.6528\n",
      "Epoch 8/10\n",
      "18112/18112 [==============================] - 284s 16ms/step - loss: 0.6031 - accuracy: 0.6769 - val_loss: 0.6377 - val_accuracy: 0.6359\n",
      "Epoch 9/10\n",
      "18112/18112 [==============================] - 295s 16ms/step - loss: 0.5994 - accuracy: 0.6811 - val_loss: 0.6392 - val_accuracy: 0.6636\n",
      "Epoch 10/10\n",
      "18112/18112 [==============================] - 295s 16ms/step - loss: 0.5962 - accuracy: 0.6824 - val_loss: 0.6033 - val_accuracy: 0.6695\n",
      "1259/1259 [==============================] - 4s 3ms/step\n",
      "Best model AUC: 0.7365322174788665\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "# Summary of the best model\n",
    "best_model.summary()\n",
    "# Optionally, you can retrain the model with the best hyperparameters on the full dataset\n",
    "best_model.fit(x_train, y_train, epochs=10, batch_size=8, validation_split=0.1)\n",
    "y_pred_test = best_model.predict(x_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "print(f'Best model AUC: {roc_auc_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN implementation\n",
    "\n",
    "# Idea: \n",
    "- 2 LSTM branches-> iteratively takes in comment tokens, and parent comment tokens\n",
    "- Dense branch-> dense layer takes in the other features\n",
    "- Merge branches\n",
    "- one more Dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_features = [\n",
    "       'comment_word_count','parent_comment_word_count',\n",
    "       'comment_token_count', 'parent_comment_token_count',\n",
    "       'comment_unique_word_count', 'parent_comment_unique_word_count',\n",
    "       'comment_unique_token_count', 'parent_comment_unique_token_count',\n",
    "       'comment_stopword_count', 'parent_comment_stopword_count',\n",
    "       'comment_mean_word_length', 'parent_comment_mean_word_length',\n",
    "       'comment_mean_token_length', 'parent_comment_mean_token_length',\n",
    "       'comment_char_count', 'parent_comment_char_count',\n",
    "       'comment_punctuation_count', 'parent_comment_punctuation_count',\n",
    "       'comment_hashtag_count', 'parent_comment_hashtag_count',\n",
    "       'comment_number_count', 'parent_comment_number_count',\n",
    "       'weighted_parent_sentiment_score_neutral',\n",
    "       'weighted_parent_sentiment_score_positive',\n",
    "       'weighted_comment_sentiment_score_neutral',\n",
    "       'weighted_comment_sentiment_score_positive', 'documents_comment']#, 'documents_parent_comment'] #if doesnt work we try comment_tdidf_nn\n",
    "train[\"documents_comment\"] = train['comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list)) #keras needs to use own tokenizer\n",
    "#train[\"documents_parent_comment\"] = train['parent_comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list))\n",
    "test[\"documents_comment\"] = test['comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list))\n",
    "#test[\"documents_parent_comment\"] = test['parent_comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list))\n",
    "\n",
    "rnn_train = train[rnn_features]\n",
    "rnn_test = test[rnn_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, TextVectorization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "# TextVectorization for comment and parent_comment\n",
    "max_features = 8000 #follow number of tokens for feedforward\n",
    "\n",
    "\n",
    "vectorize_layer_comment = TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    split='whitespace',\n",
    "    ngrams=3\n",
    "    )\n",
    "# Prepare dataset for TextVectorization adapt\n",
    "train_texts = rnn_train['documents_comment'].tolist()\n",
    "test_texts = rnn_test['documents_comment'].tolist()\n",
    "vectorize_layer_comment.adapt(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "#use tf.keras.optimizers.legacy.Adam if on M1/M2 macbook\n",
    "# Assuming max_features is the vocabulary size and embedding_dim is the dimension of the embedding\n",
    "embedding_dim = 128  # You can choose an appropriate value\n",
    "\n",
    "# Add an Embedding layer after text vectorization\n",
    "embedding_layer = Embedding(max_features, embedding_dim)\n",
    "\n",
    "# LSTM Branch\n",
    "text_input_comment = Input(shape=(1,), dtype=tf.string, name='text_comment')\n",
    "text_features_comment = vectorize_layer_comment(text_input_comment)\n",
    "text_features_comment = embedding_layer(text_features_comment)  # Embedding layer\n",
    "lstm_comment = LSTM(64)(text_features_comment)\n",
    "\n",
    "# Dense Features Branch\n",
    "other_features_input = Input(shape=(len(rnn_features) - 1,), name='other_features')\n",
    "dense_features = Dense(128, activation='relu')(other_features_input)\n",
    "\n",
    "# Concatenate\n",
    "concatenated = Concatenate()([lstm_comment, dense_features])\n",
    "\n",
    "# Additional Dense Layers\n",
    "output = Dense(64, activation='relu')(concatenated)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "# Build Model\n",
    "model = Model(inputs=[text_input_comment, other_features_input], outputs=output)\n",
    "#optimiser = tf.keras.optimizers.legacy.SGD(learning_rate=0.001)\n",
    "# Compile\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])#use Adam optimiser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare text inputs\n",
    "train_texts = np.array(train_texts)[:, np.newaxis]\n",
    "test_texts = np.array(test_texts)[:, np.newaxis]\n",
    "\n",
    "# Prepare other features inputs\n",
    "feature_columns = [col for col in rnn_features if col != 'documents_comment']\n",
    "\n",
    "train_other_features = np.array(rnn_train[feature_columns]).astype(np.float32)\n",
    "test_other_features = np.array(rnn_test[feature_columns]).astype(np.float32)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    [train_texts, train_other_features], y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1  # or use a validation set\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate([test_texts, test_other_features], y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_probs = model.predict([test_texts, test_other_features]).ravel()\n",
    "\n",
    "# Calculate ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"ROC-AUC Score: {roc_auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
