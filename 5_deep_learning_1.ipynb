{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csr_matrix, vstack, hstack\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"data/train.pkl\").dropna()\n",
    "test = pd.read_pickle(\"data/test.pkl\").dropna()\n",
    "y_train = np.array(train['label'])\n",
    "y_test = np.array(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parent_tdidf_csr = vstack(train[\"parent_comment_tdidf\"])\n",
    "test_parent_tdidf_csr = vstack(test[\"parent_comment_tdidf\"])\n",
    "\n",
    "train_tdidf_csr = vstack(train[\"comment_tdidf_nn\"])\n",
    "test_tdidf_csr = vstack(test[\"comment_tdidf_nn\"])\n",
    "\n",
    "train_parent_bow_csr = vstack(train[\"parent_comment_bow\"])\n",
    "test_parent_bow_csr = vstack(test[\"parent_comment_bow\"])\n",
    "\n",
    "train_bow_csr = vstack(train[\"comment_bow\"])\n",
    "test_bow_csr = vstack(test[\"comment_bow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<161010x8160 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 855934 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tdidf_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_features = [\n",
    "       'comment_word_count','parent_comment_word_count',\n",
    "       'comment_token_count', 'parent_comment_token_count',\n",
    "       'comment_unique_word_count', 'parent_comment_unique_word_count',\n",
    "       'comment_unique_token_count', 'parent_comment_unique_token_count',\n",
    "       'comment_stopword_count', 'parent_comment_stopword_count',\n",
    "       'comment_mean_word_length', 'parent_comment_mean_word_length',\n",
    "       'comment_mean_token_length', 'parent_comment_mean_token_length',\n",
    "       'comment_char_count', 'parent_comment_char_count',\n",
    "       'comment_punctuation_count', 'parent_comment_punctuation_count',\n",
    "       'comment_hashtag_count', 'parent_comment_hashtag_count',\n",
    "       'comment_number_count', 'parent_comment_number_count',\n",
    "       'weighted_parent_sentiment_score_neutral',\n",
    "       'weighted_parent_sentiment_score_positive',\n",
    "       'weighted_comment_sentiment_score_neutral',\n",
    "       'weighted_comment_sentiment_score_positive']\n",
    "bool_cols = ['weighted_parent_sentiment_score_neutral',\n",
    "             'weighted_parent_sentiment_score_positive',\n",
    "             'weighted_comment_sentiment_score_neutral',\n",
    "             'weighted_comment_sentiment_score_positive']\n",
    "\n",
    "for col in bool_cols: #need to convert bool type to integer\n",
    "    train[col] = train[col].astype(int)\n",
    "    test[col] = test[col].astype(int)\n",
    "X_train_gen_features = csr_matrix(train[list_of_features])\n",
    "\n",
    "X_test_gen_features = csr_matrix(test[list_of_features])\n",
    "x_train = hstack([X_train_gen_features,train_tdidf_csr]).toarray() #deep learning automates feature selection. from our supervised learning we have learnt that BoW adds no information given tf-idf.\n",
    "x_test = hstack([X_test_gen_features,test_tdidf_csr]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18112/18112 [==============================] - 58s 3ms/step - loss: 0.6692 - accuracy: 0.5858 - val_loss: 0.6569 - val_accuracy: 0.5915\n",
      "Epoch 2/10\n",
      "18112/18112 [==============================] - 55s 3ms/step - loss: 0.6233 - accuracy: 0.6489 - val_loss: 0.6263 - val_accuracy: 0.6326\n",
      "Epoch 3/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.6036 - accuracy: 0.6683 - val_loss: 0.6144 - val_accuracy: 0.6545\n",
      "Epoch 4/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5922 - accuracy: 0.6791 - val_loss: 0.5957 - val_accuracy: 0.6740\n",
      "Epoch 5/10\n",
      "18112/18112 [==============================] - 55s 3ms/step - loss: 0.5863 - accuracy: 0.6861 - val_loss: 0.5963 - val_accuracy: 0.6743\n",
      "Epoch 6/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5803 - accuracy: 0.6921 - val_loss: 0.5942 - val_accuracy: 0.6786\n",
      "Epoch 7/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5755 - accuracy: 0.6954 - val_loss: 0.5903 - val_accuracy: 0.6812\n",
      "Epoch 8/10\n",
      "18112/18112 [==============================] - 54s 3ms/step - loss: 0.5718 - accuracy: 0.6979 - val_loss: 0.5923 - val_accuracy: 0.6787\n",
      "Epoch 9/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5696 - accuracy: 0.7002 - val_loss: 0.5957 - val_accuracy: 0.6745\n",
      "Epoch 10/10\n",
      "18112/18112 [==============================] - 55s 3ms/step - loss: 0.5664 - accuracy: 0.7031 - val_loss: 0.6007 - val_accuracy: 0.6805\n",
      "1259/1259 [==============================] - 2s 1ms/step\n",
      "AUC: 0.744722806060709\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=8, validation_split=0.1)\n",
    "y_pred_test = model.predict(x_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "print(f'AUC: {roc_auc_test}')\n",
    "#comment td-idf only AUC: 0.6628971516171749\n",
    "#gen features + comment td-idf AUC: 0.7419122793471202\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using iterative hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "from keras_tuner import HyperModel\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#use tf.keras.optimizers.legacy.Adam if on M1/M2 macbook\n",
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        # First layer\n",
    "        model.add(Dense(units=hp.Int('units_first', min_value=128, max_value=512, step=32),\n",
    "                        activation='relu', input_dim=self.input_dim))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_first', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "        \n",
    "        # Second layer\n",
    "        model.add(Dense(units=hp.Int('units_second', min_value=64, max_value=256, step=32),\n",
    "                        activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_second', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Third layer\n",
    "        model.add(Dense(units=hp.Int('units_third', min_value=32, max_value=128, step=32),\n",
    "                        activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_third', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Tuning the learning rate\n",
    "        hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "        \n",
    "        model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from keras_tuner_dir\\keras_tuner\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cmz\\AppData\\Local\\Temp\\ipykernel_19772\\215952752.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import Hyperband\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "hypermodel = MyHyperModel(input_dim=x_train.shape[1])\n",
    "\n",
    "tuner = Hyperband(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='keras_tuner'\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(x_train, y_train,\n",
    "             epochs=50,\n",
    "             validation_split=0.1,\n",
    "             callbacks=[stop_early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 352)               2860000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 352)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 160)               56480     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 160)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                10304     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2926849 (11.17 MB)\n",
      "Trainable params: 2926849 (11.17 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "18112/18112 [==============================] - 288s 16ms/step - loss: 0.6918 - accuracy: 0.5392 - val_loss: 0.6800 - val_accuracy: 0.5556\n",
      "Epoch 2/10\n",
      "18112/18112 [==============================] - 284s 16ms/step - loss: 0.6771 - accuracy: 0.5738 - val_loss: 0.6659 - val_accuracy: 0.5992\n",
      "Epoch 3/10\n",
      "18112/18112 [==============================] - 284s 16ms/step - loss: 0.6541 - accuracy: 0.6188 - val_loss: 0.6265 - val_accuracy: 0.6534\n",
      "Epoch 4/10\n",
      "18112/18112 [==============================] - 282s 16ms/step - loss: 0.6308 - accuracy: 0.6509 - val_loss: 0.6173 - val_accuracy: 0.6649\n",
      "Epoch 5/10\n",
      "18112/18112 [==============================] - 285s 16ms/step - loss: 0.6232 - accuracy: 0.6596 - val_loss: 0.6367 - val_accuracy: 0.6330\n",
      "Epoch 6/10\n",
      "18112/18112 [==============================] - 285s 16ms/step - loss: 0.6126 - accuracy: 0.6684 - val_loss: 0.6020 - val_accuracy: 0.6760\n",
      "Epoch 7/10\n",
      "18112/18112 [==============================] - 283s 16ms/step - loss: 0.6059 - accuracy: 0.6722 - val_loss: 0.6136 - val_accuracy: 0.6528\n",
      "Epoch 8/10\n",
      "18112/18112 [==============================] - 284s 16ms/step - loss: 0.6031 - accuracy: 0.6769 - val_loss: 0.6377 - val_accuracy: 0.6359\n",
      "Epoch 9/10\n",
      "18112/18112 [==============================] - 295s 16ms/step - loss: 0.5994 - accuracy: 0.6811 - val_loss: 0.6392 - val_accuracy: 0.6636\n",
      "Epoch 10/10\n",
      "18112/18112 [==============================] - 295s 16ms/step - loss: 0.5962 - accuracy: 0.6824 - val_loss: 0.6033 - val_accuracy: 0.6695\n",
      "1259/1259 [==============================] - 4s 3ms/step\n",
      "Best model AUC: 0.7365322174788665\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "# Summary of the best model\n",
    "best_model.summary()\n",
    "# Optionally, you can retrain the model with the best hyperparameters on the full dataset\n",
    "best_model.fit(x_train, y_train, epochs=10, batch_size=8, validation_split=0.1)\n",
    "y_pred_test = best_model.predict(x_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "print(f'Best model AUC: {roc_auc_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN implementation\n",
    "\n",
    "# Idea: \n",
    "- 2 LSTM branches-> iteratively takes in comment tokens, and parent comment tokens\n",
    "- Dense branch-> dense layer takes in the other features\n",
    "- Merge branches\n",
    "- one more Dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_features = [\n",
    "       'comment_word_count','parent_comment_word_count',\n",
    "       'comment_token_count', 'parent_comment_token_count',\n",
    "       'comment_unique_word_count', 'parent_comment_unique_word_count',\n",
    "       'comment_unique_token_count', 'parent_comment_unique_token_count',\n",
    "       'comment_stopword_count', 'parent_comment_stopword_count',\n",
    "       'comment_mean_word_length', 'parent_comment_mean_word_length',\n",
    "       'comment_mean_token_length', 'parent_comment_mean_token_length',\n",
    "       'comment_char_count', 'parent_comment_char_count',\n",
    "       'comment_punctuation_count', 'parent_comment_punctuation_count',\n",
    "       'comment_hashtag_count', 'parent_comment_hashtag_count',\n",
    "       'comment_number_count', 'parent_comment_number_count',\n",
    "       'weighted_parent_sentiment_score_neutral',\n",
    "       'weighted_parent_sentiment_score_positive',\n",
    "       'weighted_comment_sentiment_score_neutral',\n",
    "       'weighted_comment_sentiment_score_positive', 'documents_comment']#, 'documents_parent_comment'] #if doesnt work we try comment_tdidf_nn\n",
    "train[\"documents_comment\"] = train['comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list)) #keras needs to use own tokenizer\n",
    "#train[\"documents_parent_comment\"] = train['parent_comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list))\n",
    "test[\"documents_comment\"] = test['comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list))\n",
    "#test[\"documents_parent_comment\"] = test['parent_comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list))\n",
    "\n",
    "rnn_train = train[rnn_features]\n",
    "rnn_test = test[rnn_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, TextVectorization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "# TextVectorization for comment and parent_comment\n",
    "max_features = 8000 #follow number of tokens for feedforward\n",
    "\n",
    "\n",
    "vectorize_layer_comment = TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    split='whitespace',\n",
    "    ngrams=3\n",
    "    )\n",
    "# Prepare dataset for TextVectorization adapt\n",
    "train_texts = rnn_train['documents_comment'].tolist()\n",
    "test_texts = rnn_test['documents_comment'].tolist()\n",
    "vectorize_layer_comment.adapt(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "#use tf.keras.optimizers.legacy.Adam if on M1/M2 macbook\n",
    "# Assuming max_features is the vocabulary size and embedding_dim is the dimension of the embedding\n",
    "embedding_dim = 128  # You can choose an appropriate value\n",
    "\n",
    "# Add an Embedding layer after text vectorization\n",
    "embedding_layer = Embedding(max_features, embedding_dim)\n",
    "\n",
    "# LSTM Branch\n",
    "text_input_comment = Input(shape=(1,), dtype=tf.string, name='text_comment')\n",
    "text_features_comment = vectorize_layer_comment(text_input_comment)\n",
    "text_features_comment = embedding_layer(text_features_comment)  # Embedding layer\n",
    "lstm_comment = LSTM(64)(text_features_comment)\n",
    "\n",
    "# Dense Features Branch\n",
    "other_features_input = Input(shape=(len(rnn_features) - 1,), name='other_features')\n",
    "dense_features = Dense(128, activation='relu')(other_features_input)\n",
    "\n",
    "# Concatenate\n",
    "concatenated = Concatenate()([lstm_comment, dense_features])\n",
    "\n",
    "# Additional Dense Layers\n",
    "output = Dense(64, activation='relu')(concatenated)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "# Build Model\n",
    "model = Model(inputs=[text_input_comment, other_features_input], outputs=output)\n",
    "#optimiser = tf.keras.optimizers.legacy.SGD(learning_rate=0.001)\n",
    "# Compile\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])#use Adam optimiser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "3774/3774 [==============================] - 104s 27ms/step - loss: 0.7698 - accuracy: 0.5429 - val_loss: 0.6793 - val_accuracy: 0.5651\n",
      "Epoch 2/40\n",
      "3774/3774 [==============================] - 98s 26ms/step - loss: 0.6876 - accuracy: 0.5586 - val_loss: 0.6800 - val_accuracy: 0.5580\n",
      "Epoch 3/40\n",
      "3774/3774 [==============================] - 101s 27ms/step - loss: 0.6844 - accuracy: 0.5662 - val_loss: 0.6767 - val_accuracy: 0.5656\n",
      "Epoch 4/40\n",
      "3774/3774 [==============================] - 102s 27ms/step - loss: 0.6766 - accuracy: 0.5671 - val_loss: 0.6774 - val_accuracy: 0.5625\n",
      "Epoch 5/40\n",
      "3774/3774 [==============================] - 99s 26ms/step - loss: 0.6753 - accuracy: 0.5698 - val_loss: 0.6753 - val_accuracy: 0.5688\n",
      "Epoch 6/40\n",
      "3774/3774 [==============================] - 95s 25ms/step - loss: 0.6748 - accuracy: 0.5710 - val_loss: 0.6739 - val_accuracy: 0.5711\n",
      "Epoch 7/40\n",
      "3774/3774 [==============================] - 96s 25ms/step - loss: 0.6829 - accuracy: 0.5703 - val_loss: 0.6900 - val_accuracy: 0.5539\n",
      "Epoch 8/40\n",
      "3774/3774 [==============================] - 99s 26ms/step - loss: 0.6745 - accuracy: 0.5742 - val_loss: 0.6733 - val_accuracy: 0.5749\n",
      "Epoch 9/40\n",
      "3774/3774 [==============================] - 97s 26ms/step - loss: 0.6732 - accuracy: 0.5733 - val_loss: 0.6768 - val_accuracy: 0.5601\n",
      "Epoch 10/40\n",
      "3774/3774 [==============================] - 96s 25ms/step - loss: 0.6548 - accuracy: 0.6074 - val_loss: 0.6244 - val_accuracy: 0.6502\n",
      "Epoch 11/40\n",
      "3774/3774 [==============================] - 94s 25ms/step - loss: 0.5964 - accuracy: 0.6817 - val_loss: 0.5977 - val_accuracy: 0.6762\n",
      "Epoch 12/40\n",
      "3774/3774 [==============================] - 98s 26ms/step - loss: 0.5582 - accuracy: 0.7132 - val_loss: 0.6040 - val_accuracy: 0.6769\n",
      "Epoch 13/40\n",
      "3774/3774 [==============================] - 97s 26ms/step - loss: 0.5280 - accuracy: 0.7367 - val_loss: 0.6048 - val_accuracy: 0.6717\n",
      "Epoch 14/40\n",
      "3774/3774 [==============================] - 93s 25ms/step - loss: 0.4970 - accuracy: 0.7575 - val_loss: 0.6206 - val_accuracy: 0.6707\n",
      "Epoch 15/40\n",
      "3774/3774 [==============================] - 93s 25ms/step - loss: 0.4623 - accuracy: 0.7804 - val_loss: 0.6631 - val_accuracy: 0.6604\n",
      "Epoch 16/40\n",
      "3774/3774 [==============================] - 99s 26ms/step - loss: 0.4264 - accuracy: 0.8021 - val_loss: 0.7115 - val_accuracy: 0.6535\n",
      "Epoch 17/40\n",
      "3774/3774 [==============================] - 97s 26ms/step - loss: 0.3906 - accuracy: 0.8211 - val_loss: 0.7274 - val_accuracy: 0.6506\n",
      "Epoch 18/40\n",
      "3774/3774 [==============================] - 97s 26ms/step - loss: 0.3579 - accuracy: 0.8389 - val_loss: 0.7935 - val_accuracy: 0.6496\n",
      "Epoch 19/40\n",
      "3774/3774 [==============================] - 97s 26ms/step - loss: 0.3269 - accuracy: 0.8544 - val_loss: 0.8433 - val_accuracy: 0.6446\n",
      "Epoch 20/40\n",
      "3774/3774 [==============================] - 97s 26ms/step - loss: 0.3005 - accuracy: 0.8666 - val_loss: 0.9293 - val_accuracy: 0.6438\n",
      "Epoch 21/40\n",
      "3774/3774 [==============================] - 101s 27ms/step - loss: 0.2800 - accuracy: 0.8772 - val_loss: 0.9348 - val_accuracy: 0.6425\n",
      "Epoch 22/40\n",
      "3774/3774 [==============================] - 99s 26ms/step - loss: 0.2595 - accuracy: 0.8879 - val_loss: 1.0309 - val_accuracy: 0.6387\n",
      "Epoch 23/40\n",
      "3774/3774 [==============================] - 97s 26ms/step - loss: 0.2417 - accuracy: 0.8955 - val_loss: 1.0455 - val_accuracy: 0.6386\n",
      "Epoch 24/40\n",
      "3774/3774 [==============================] - 97s 26ms/step - loss: 0.2269 - accuracy: 0.9027 - val_loss: 1.1377 - val_accuracy: 0.6362\n",
      "Epoch 25/40\n",
      "3774/3774 [==============================] - 99s 26ms/step - loss: 0.2114 - accuracy: 0.9098 - val_loss: 1.1548 - val_accuracy: 0.6353\n",
      "Epoch 26/40\n",
      "3774/3774 [==============================] - 95s 25ms/step - loss: 0.2019 - accuracy: 0.9139 - val_loss: 1.2042 - val_accuracy: 0.6321\n",
      "Epoch 27/40\n",
      "3774/3774 [==============================] - 97s 26ms/step - loss: 0.1913 - accuracy: 0.9180 - val_loss: 1.3042 - val_accuracy: 0.6332\n",
      "Epoch 28/40\n",
      "3774/3774 [==============================] - 95s 25ms/step - loss: 0.1867 - accuracy: 0.9218 - val_loss: 1.3101 - val_accuracy: 0.6311\n",
      "Epoch 29/40\n",
      "3774/3774 [==============================] - 94s 25ms/step - loss: 0.1721 - accuracy: 0.9259 - val_loss: 1.4135 - val_accuracy: 0.6317\n",
      "Epoch 30/40\n",
      "3774/3774 [==============================] - 95s 25ms/step - loss: 0.1649 - accuracy: 0.9283 - val_loss: 1.4154 - val_accuracy: 0.6308\n",
      "Epoch 31/40\n",
      "3774/3774 [==============================] - 95s 25ms/step - loss: 0.1568 - accuracy: 0.9319 - val_loss: 1.4011 - val_accuracy: 0.6294\n",
      "Epoch 32/40\n",
      "3774/3774 [==============================] - 96s 26ms/step - loss: 0.1507 - accuracy: 0.9346 - val_loss: 1.5051 - val_accuracy: 0.6276\n",
      "Epoch 33/40\n",
      "3774/3774 [==============================] - 94s 25ms/step - loss: 0.1540 - accuracy: 0.9356 - val_loss: 1.5053 - val_accuracy: 0.6296\n",
      "Epoch 34/40\n",
      "3774/3774 [==============================] - 93s 25ms/step - loss: 0.1408 - accuracy: 0.9382 - val_loss: 1.6429 - val_accuracy: 0.6261\n",
      "Epoch 35/40\n",
      "3774/3774 [==============================] - 97s 26ms/step - loss: 0.1394 - accuracy: 0.9389 - val_loss: 1.5885 - val_accuracy: 0.6308\n",
      "Epoch 36/40\n",
      "3774/3774 [==============================] - 96s 25ms/step - loss: 0.1340 - accuracy: 0.9407 - val_loss: 1.6793 - val_accuracy: 0.6298\n",
      "Epoch 37/40\n",
      "3774/3774 [==============================] - 94s 25ms/step - loss: 0.1325 - accuracy: 0.9421 - val_loss: 1.6383 - val_accuracy: 0.6293\n",
      "Epoch 38/40\n",
      "3774/3774 [==============================] - 95s 25ms/step - loss: 0.1265 - accuracy: 0.9442 - val_loss: 1.7926 - val_accuracy: 0.6306\n",
      "Epoch 39/40\n",
      "3774/3774 [==============================] - 98s 26ms/step - loss: 0.1270 - accuracy: 0.9443 - val_loss: 1.6503 - val_accuracy: 0.6282\n",
      "Epoch 40/40\n",
      "3774/3774 [==============================] - 97s 26ms/step - loss: 0.1220 - accuracy: 0.9456 - val_loss: 1.7080 - val_accuracy: 0.6295\n",
      "1259/1259 [==============================] - 7s 6ms/step - loss: 1.6900 - accuracy: 0.6306\n",
      "Test Loss: 1.6900098323822021, Test Accuracy: 0.6306102871894836\n",
      "1259/1259 [==============================] - 7s 6ms/step\n",
      "ROC-AUC Score: 0.6599739705156451\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare text inputs\n",
    "train_texts = np.array(train_texts)[:, np.newaxis]\n",
    "test_texts = np.array(test_texts)[:, np.newaxis]\n",
    "\n",
    "# Prepare other features inputs\n",
    "feature_columns = [col for col in rnn_features if col != 'documents_comment']\n",
    "\n",
    "train_other_features = np.array(rnn_train[feature_columns]).astype(np.float32)\n",
    "test_other_features = np.array(rnn_test[feature_columns]).astype(np.float32)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    [train_texts, train_other_features], y_train,\n",
    "    epochs=40,\n",
    "    batch_size=32,\n",
    "    validation_split=0.25  # or use a validation set\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate([test_texts, test_other_features], y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_probs = model.predict([test_texts, test_other_features]).ravel()\n",
    "\n",
    "# Calculate ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"ROC-AUC Score: {roc_auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
