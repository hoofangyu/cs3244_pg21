{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csr_matrix, vstack, hstack\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"data/train.pkl\").dropna()\n",
    "test = pd.read_pickle(\"data/test.pkl\").dropna()\n",
    "y_train = np.array(train['label'])\n",
    "y_test = np.array(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parent_tdidf_csr = vstack(train[\"parent_comment_tdidf\"])\n",
    "test_parent_tdidf_csr = vstack(test[\"parent_comment_tdidf\"])\n",
    "\n",
    "train_tdidf_csr = vstack(train[\"comment_tdidf_nn\"])\n",
    "test_tdidf_csr = vstack(test[\"comment_tdidf_nn\"])\n",
    "\n",
    "train_parent_bow_csr = vstack(train[\"parent_comment_bow\"])\n",
    "test_parent_bow_csr = vstack(test[\"parent_comment_bow\"])\n",
    "\n",
    "train_bow_csr = vstack(train[\"comment_bow\"])\n",
    "test_bow_csr = vstack(test[\"comment_bow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<161010x8160 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 855934 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tdidf_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_features = [\n",
    "       'comment_word_count','parent_comment_word_count',\n",
    "       'comment_token_count', 'parent_comment_token_count',\n",
    "       'comment_unique_word_count', 'parent_comment_unique_word_count',\n",
    "       'comment_unique_token_count', 'parent_comment_unique_token_count',\n",
    "       'comment_stopword_count', 'parent_comment_stopword_count',\n",
    "       'comment_mean_word_length', 'parent_comment_mean_word_length',\n",
    "       'comment_mean_token_length', 'parent_comment_mean_token_length',\n",
    "       'comment_char_count', 'parent_comment_char_count',\n",
    "       'comment_punctuation_count', 'parent_comment_punctuation_count',\n",
    "       'comment_hashtag_count', 'parent_comment_hashtag_count',\n",
    "       'comment_number_count', 'parent_comment_number_count',\n",
    "       'weighted_parent_sentiment_score_neutral',\n",
    "       'weighted_parent_sentiment_score_positive',\n",
    "       'weighted_comment_sentiment_score_neutral',\n",
    "       'weighted_comment_sentiment_score_positive']\n",
    "bool_cols = ['weighted_parent_sentiment_score_neutral',\n",
    "             'weighted_parent_sentiment_score_positive',\n",
    "             'weighted_comment_sentiment_score_neutral',\n",
    "             'weighted_comment_sentiment_score_positive']\n",
    "\n",
    "for col in bool_cols: #need to convert bool type to integer\n",
    "    train[col] = train[col].astype(int)\n",
    "    test[col] = test[col].astype(int)\n",
    "X_train_gen_features = csr_matrix(train[list_of_features])\n",
    "\n",
    "X_test_gen_features = csr_matrix(test[list_of_features])\n",
    "x_train = hstack([X_train_gen_features,train_tdidf_csr]).toarray() #deep learning automates feature selection. from our supervised learning we have learnt that BoW adds no information given tf-idf.\n",
    "x_test = hstack([X_test_gen_features,test_tdidf_csr]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18112/18112 [==============================] - 58s 3ms/step - loss: 0.6692 - accuracy: 0.5858 - val_loss: 0.6569 - val_accuracy: 0.5915\n",
      "Epoch 2/10\n",
      "18112/18112 [==============================] - 55s 3ms/step - loss: 0.6233 - accuracy: 0.6489 - val_loss: 0.6263 - val_accuracy: 0.6326\n",
      "Epoch 3/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.6036 - accuracy: 0.6683 - val_loss: 0.6144 - val_accuracy: 0.6545\n",
      "Epoch 4/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5922 - accuracy: 0.6791 - val_loss: 0.5957 - val_accuracy: 0.6740\n",
      "Epoch 5/10\n",
      "18112/18112 [==============================] - 55s 3ms/step - loss: 0.5863 - accuracy: 0.6861 - val_loss: 0.5963 - val_accuracy: 0.6743\n",
      "Epoch 6/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5803 - accuracy: 0.6921 - val_loss: 0.5942 - val_accuracy: 0.6786\n",
      "Epoch 7/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5755 - accuracy: 0.6954 - val_loss: 0.5903 - val_accuracy: 0.6812\n",
      "Epoch 8/10\n",
      "18112/18112 [==============================] - 54s 3ms/step - loss: 0.5718 - accuracy: 0.6979 - val_loss: 0.5923 - val_accuracy: 0.6787\n",
      "Epoch 9/10\n",
      "18112/18112 [==============================] - 53s 3ms/step - loss: 0.5696 - accuracy: 0.7002 - val_loss: 0.5957 - val_accuracy: 0.6745\n",
      "Epoch 10/10\n",
      "18112/18112 [==============================] - 55s 3ms/step - loss: 0.5664 - accuracy: 0.7031 - val_loss: 0.6007 - val_accuracy: 0.6805\n",
      "1259/1259 [==============================] - 2s 1ms/step\n",
      "AUC: 0.744722806060709\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=8, validation_split=0.1)\n",
    "y_pred_test = model.predict(x_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "print(f'AUC: {roc_auc_test}')\n",
    "#comment td-idf only AUC: 0.6628971516171749\n",
    "#gen features + comment td-idf AUC: 0.7419122793471202\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using iterative hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import HyperModel\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#use tf.keras.optimizers.legacy.Adam if on M1/M2 macbook\n",
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        # First layer\n",
    "        model.add(Dense(units=hp.Int('units_first', min_value=128, max_value=512, step=32),\n",
    "                        activation='relu', input_dim=self.input_dim))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_first', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "        \n",
    "        # Second layer\n",
    "        model.add(Dense(units=hp.Int('units_second', min_value=64, max_value=256, step=32),\n",
    "                        activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_second', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Third layer\n",
    "        model.add(Dense(units=hp.Int('units_third', min_value=32, max_value=128, step=32),\n",
    "                        activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_third', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Tuning the learning rate\n",
    "        hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "        \n",
    "        model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from keras_tuner_dir\\keras_tuner\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cmz\\AppData\\Local\\Temp\\ipykernel_19772\\215952752.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import Hyperband\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "hypermodel = MyHyperModel(input_dim=x_train.shape[1])\n",
    "\n",
    "tuner = Hyperband(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='keras_tuner'\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(x_train, y_train,\n",
    "             epochs=50,\n",
    "             validation_split=0.1,\n",
    "             callbacks=[stop_early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 352)               2860000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 352)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 160)               56480     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 160)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                10304     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2926849 (11.17 MB)\n",
      "Trainable params: 2926849 (11.17 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "18112/18112 [==============================] - 288s 16ms/step - loss: 0.6918 - accuracy: 0.5392 - val_loss: 0.6800 - val_accuracy: 0.5556\n",
      "Epoch 2/10\n",
      "18112/18112 [==============================] - 284s 16ms/step - loss: 0.6771 - accuracy: 0.5738 - val_loss: 0.6659 - val_accuracy: 0.5992\n",
      "Epoch 3/10\n",
      "18112/18112 [==============================] - 284s 16ms/step - loss: 0.6541 - accuracy: 0.6188 - val_loss: 0.6265 - val_accuracy: 0.6534\n",
      "Epoch 4/10\n",
      "18112/18112 [==============================] - 282s 16ms/step - loss: 0.6308 - accuracy: 0.6509 - val_loss: 0.6173 - val_accuracy: 0.6649\n",
      "Epoch 5/10\n",
      "18112/18112 [==============================] - 285s 16ms/step - loss: 0.6232 - accuracy: 0.6596 - val_loss: 0.6367 - val_accuracy: 0.6330\n",
      "Epoch 6/10\n",
      "18112/18112 [==============================] - 285s 16ms/step - loss: 0.6126 - accuracy: 0.6684 - val_loss: 0.6020 - val_accuracy: 0.6760\n",
      "Epoch 7/10\n",
      "18112/18112 [==============================] - 283s 16ms/step - loss: 0.6059 - accuracy: 0.6722 - val_loss: 0.6136 - val_accuracy: 0.6528\n",
      "Epoch 8/10\n",
      "18112/18112 [==============================] - 284s 16ms/step - loss: 0.6031 - accuracy: 0.6769 - val_loss: 0.6377 - val_accuracy: 0.6359\n",
      "Epoch 9/10\n",
      "18112/18112 [==============================] - 295s 16ms/step - loss: 0.5994 - accuracy: 0.6811 - val_loss: 0.6392 - val_accuracy: 0.6636\n",
      "Epoch 10/10\n",
      "18112/18112 [==============================] - 295s 16ms/step - loss: 0.5962 - accuracy: 0.6824 - val_loss: 0.6033 - val_accuracy: 0.6695\n",
      "1259/1259 [==============================] - 4s 3ms/step\n",
      "Best model AUC: 0.7365322174788665\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "# Summary of the best model\n",
    "best_model.summary()\n",
    "# Optionally, you can retrain the model with the best hyperparameters on the full dataset\n",
    "best_model.fit(x_train, y_train, epochs=10, batch_size=8, validation_split=0.1)\n",
    "y_pred_test = best_model.predict(x_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "print(f'Best model AUC: {roc_auc_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN implementation\n",
    "\n",
    "# Idea: \n",
    "- 2 LSTM branches-> iteratively takes in comment tokens, and parent comment tokens\n",
    "- Dense branch-> dense layer takes in the other features\n",
    "- Merge branches\n",
    "- one more Dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_features = [\n",
    "       'comment_word_count','parent_comment_word_count',\n",
    "       'comment_token_count', 'parent_comment_token_count',\n",
    "       'comment_unique_word_count', 'parent_comment_unique_word_count',\n",
    "       'comment_unique_token_count', 'parent_comment_unique_token_count',\n",
    "       'comment_stopword_count', 'parent_comment_stopword_count',\n",
    "       'comment_mean_word_length', 'parent_comment_mean_word_length',\n",
    "       'comment_mean_token_length', 'parent_comment_mean_token_length',\n",
    "       'comment_char_count', 'parent_comment_char_count',\n",
    "       'comment_punctuation_count', 'parent_comment_punctuation_count',\n",
    "       'comment_hashtag_count', 'parent_comment_hashtag_count',\n",
    "       'comment_number_count', 'parent_comment_number_count',\n",
    "       'weighted_parent_sentiment_score_neutral',\n",
    "       'weighted_parent_sentiment_score_positive',\n",
    "       'weighted_comment_sentiment_score_neutral',\n",
    "       'weighted_comment_sentiment_score_positive', 'documents_comment', 'documents_parent_comment'] #if doesnt work we try comment_tdidf_nn\n",
    "train[\"documents_comment\"] = train['comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list)) #keras needs to use own tokenizer\n",
    "train[\"documents_parent_comment\"] = train['parent_comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list))\n",
    "test[\"documents_comment\"] = test['comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list))\n",
    "test[\"documents_parent_comment\"] = test['parent_comment_tokens'].apply(lambda tokens_list:' '.join(tokens_list))\n",
    "\n",
    "rnn_train = train[rnn_features]\n",
    "rnn_test = test[rnn_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Prepare dataset for TextVectorization adapt\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m train_texts \u001b[39m=\u001b[39m rnn_train[\u001b[39m'\u001b[39m\u001b[39mdocuments_comment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist() \u001b[39m+\u001b[39m rnn_train[\u001b[39m'\u001b[39m\u001b[39mdocuments_parent_comment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m vectorize_layer_comment\u001b[39m.\u001b[39;49madapt(train_texts)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m vectorize_layer_parent_comment\u001b[39m.\u001b[39madapt(train_texts)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/preprocessing/text_vectorization.py:473\u001b[0m, in \u001b[0;36mTextVectorization.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madapt\u001b[39m(\u001b[39mself\u001b[39m, data, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, steps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    424\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Computes a vocabulary of string terms from tokens in a dataset.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[39m    Calling `adapt()` on a `TextVectorization` layer is an alternative to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39m          argument is not supported with array inputs.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49madapt(data, batch_size\u001b[39m=\u001b[39;49mbatch_size, steps\u001b[39m=\u001b[39;49msteps)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/base_preprocessing_layer.py:246\u001b[0m, in \u001b[0;36mPreprocessingLayer.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m    245\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_state()\n\u001b[0;32m--> 246\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mDataHandler(\n\u001b[1;32m    247\u001b[0m     data,\n\u001b[1;32m    248\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    249\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m    250\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    251\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[1;32m    252\u001b[0m     distribute\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    253\u001b[0m )\n\u001b[1;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapt_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_adapt_function()\n\u001b[1;32m    255\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1292\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m   1291\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   1293\u001b[0m     x,\n\u001b[1;32m   1294\u001b[0m     y,\n\u001b[1;32m   1295\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1296\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   1297\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[1;32m   1298\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1299\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1300\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1301\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1302\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1303\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[1;32m   1304\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1305\u001b[0m     pss_evaluation_shards\u001b[39m=\u001b[39;49mpss_evaluation_shards,\n\u001b[1;32m   1306\u001b[0m )\n\u001b[1;32m   1308\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:714\u001b[0m, in \u001b[0;36mListsOfScalarsDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    704\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    705\u001b[0m     x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    712\u001b[0m ):\n\u001b[1;32m    713\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(x, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 714\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(x)\n\u001b[1;32m    715\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m         y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, TextVectorization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "# TextVectorization for comment and parent_comment\n",
    "max_features = 8000 #follow number of tokens for feedforward\n",
    "\n",
    "\n",
    "vectorize_layer_comment = TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    split='whitespace',\n",
    "    ngrams=3\n",
    "    )\n",
    "vectorize_layer_parent_comment = TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    split='whitespace',\n",
    "    ngrams=3\n",
    "    )\n",
    "# Prepare dataset for TextVectorization adapt\n",
    "train_texts = rnn_train['documents_comment'].tolist() + rnn_train['documents_parent_comment'].tolist()\n",
    "vectorize_layer_comment.adapt(train_texts)\n",
    "vectorize_layer_parent_comment.adapt(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Prepare dataset for TextVectorization adapt\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m train_texts \u001b[39m=\u001b[39m rnn_train[\u001b[39m'\u001b[39m\u001b[39mdocuments_comment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist() \u001b[39m+\u001b[39m rnn_train[\u001b[39m'\u001b[39m\u001b[39mdocuments_parent_comment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m vectorize_layer_comment\u001b[39m.\u001b[39;49madapt(train_texts)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X24sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m vectorize_layer_parent_comment\u001b[39m.\u001b[39madapt(train_texts)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ethan/Documents/Documents/Y3S1/CS3244/src/cs3244_pg21/5_deep_learning_1.ipynb#X24sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# LSTM Branch\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/preprocessing/text_vectorization.py:473\u001b[0m, in \u001b[0;36mTextVectorization.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madapt\u001b[39m(\u001b[39mself\u001b[39m, data, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, steps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    424\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Computes a vocabulary of string terms from tokens in a dataset.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[39m    Calling `adapt()` on a `TextVectorization` layer is an alternative to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39m          argument is not supported with array inputs.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49madapt(data, batch_size\u001b[39m=\u001b[39;49mbatch_size, steps\u001b[39m=\u001b[39;49msteps)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/base_preprocessing_layer.py:246\u001b[0m, in \u001b[0;36mPreprocessingLayer.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m    245\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_state()\n\u001b[0;32m--> 246\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mDataHandler(\n\u001b[1;32m    247\u001b[0m     data,\n\u001b[1;32m    248\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    249\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m    250\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    251\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[1;32m    252\u001b[0m     distribute\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    253\u001b[0m )\n\u001b[1;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapt_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_adapt_function()\n\u001b[1;32m    255\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1292\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m   1291\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   1293\u001b[0m     x,\n\u001b[1;32m   1294\u001b[0m     y,\n\u001b[1;32m   1295\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1296\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   1297\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[1;32m   1298\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1299\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1300\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1301\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1302\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1303\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[1;32m   1304\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1305\u001b[0m     pss_evaluation_shards\u001b[39m=\u001b[39;49mpss_evaluation_shards,\n\u001b[1;32m   1306\u001b[0m )\n\u001b[1;32m   1308\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:714\u001b[0m, in \u001b[0;36mListsOfScalarsDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    704\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    705\u001b[0m     x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    712\u001b[0m ):\n\u001b[1;32m    713\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(x, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 714\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(x)\n\u001b[1;32m    715\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m         y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LSTM Branch\n",
    "text_input_comment = Input(shape=(1,), dtype=tf.string, name='text_comment')\n",
    "text_input_parent_comment = Input(shape=(1,), dtype=tf.string, name='text_parent_comment')\n",
    "\n",
    "text_features_comment = vectorize_layer_comment(text_input_comment)\n",
    "text_features_parent_comment = vectorize_layer_parent_comment(text_input_parent_comment)\n",
    "\n",
    "lstm_comment = LSTM(64)(text_features_comment)\n",
    "lstm_parent_comment = LSTM(64)(text_features_parent_comment)\n",
    "\n",
    "# Dense Features Branch\n",
    "other_features_input = Input(shape=(len(rnn_features) - 2,), name='other_features')\n",
    "dense_features = Dense(128, activation='relu')(other_features_input)\n",
    "\n",
    "# Concatenate\n",
    "concatenated = Concatenate()([lstm_comment, lstm_parent_comment, dense_features])\n",
    "\n",
    "# Additional Dense Layers\n",
    "output = Dense(64, activation='relu')(concatenated)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "# Build Model\n",
    "model = Model(inputs=[text_input_comment, text_input_parent_comment, other_features_input], outputs=output)\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "# Note: You'll need to extract and prepare the input data accordingly\n",
    "# model.fit([train_comments, train_parent_comments, train_other_features], y_train, epochs=10, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
