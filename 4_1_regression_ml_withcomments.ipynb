{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5LshF02lv8i"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "Import necessatry libraries to perform logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nouHCGTIlv8l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_auc_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from scipy.sparse import csr_matrix, vstack, hstack\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtsW9pTJlv8m"
      },
      "source": [
        "### Data Prep for ML\n",
        "\n",
        "Unpickle data and remove any NA values.\n",
        "\n",
        "Concate \"comment\" and \"parent_comment\" for TD-idf and BoW from training and testing datasets using vstack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ernneS2lv8m"
      },
      "outputs": [],
      "source": [
        "train = pd.read_pickle(\"data/train.pkl\").dropna()\n",
        "test = pd.read_pickle(\"data/test.pkl\").dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWuVYbySlv8n"
      },
      "outputs": [],
      "source": [
        "train_parent_tdidf_csr = vstack(train[\"parent_comment_tdidf\"])\n",
        "test_parent_tdidf_csr = vstack(test[\"parent_comment_tdidf\"])\n",
        "\n",
        "train_tdidf_csr = vstack(train[\"comment_tdidf\"])\n",
        "test_tdidf_csr = vstack(test[\"comment_tdidf\"])\n",
        "\n",
        "train_parent_bow_csr = vstack(train[\"parent_comment_bow\"])\n",
        "test_parent_bow_csr = vstack(test[\"parent_comment_bow\"])\n",
        "\n",
        "train_bow_csr = vstack(train[\"comment_bow\"])\n",
        "test_bow_csr = vstack(test[\"comment_bow\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAkau9lJlv8n",
        "outputId": "eee0da74-1202-4471-a354-e2d8aaf6677a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['comment', 'parent_comment', 'comment_tokens', 'parent_comment_tokens',\n",
              "       'comment_tokens_bigram', 'parent_comment_tokens_bigram',\n",
              "       'comment_tokens_trigram', 'parent_comment_tokens_trigram',\n",
              "       'comment_score', 'parent_comment_score',\n",
              "       'weighted_parent_sentiment_score_neutral',\n",
              "       'weighted_parent_sentiment_score_positive',\n",
              "       'weighted_comment_sentiment_score_neutral',\n",
              "       'weighted_comment_sentiment_score_positive', 'comment_word_count',\n",
              "       'parent_comment_word_count', 'comment_token_count',\n",
              "       'parent_comment_token_count', 'comment_unique_word_count',\n",
              "       'parent_comment_unique_word_count', 'comment_unique_token_count',\n",
              "       'parent_comment_unique_token_count', 'comment_stopword_count',\n",
              "       'parent_comment_stopword_count', 'comment_mean_word_length',\n",
              "       'parent_comment_mean_word_length', 'comment_mean_token_length',\n",
              "       'parent_comment_mean_token_length', 'comment_char_count',\n",
              "       'parent_comment_char_count', 'comment_punctuation_count',\n",
              "       'parent_comment_punctuation_count', 'comment_hashtag_count',\n",
              "       'parent_comment_hashtag_count', 'comment_number_count',\n",
              "       'parent_comment_number_count', 'comment_bow', 'parent_comment_bow',\n",
              "       'comment_tdidf', 'parent_comment_tdidf', 'label'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of features necessary for logistic regression."
      ],
      "metadata": {
        "id": "3d5I1PHHm0DJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg6gX4-3lv8o"
      },
      "outputs": [],
      "source": [
        "list_of_features = [\n",
        "       'comment_word_count','parent_comment_word_count',\n",
        "       'comment_token_count', 'parent_comment_token_count',\n",
        "       'comment_unique_word_count', 'parent_comment_unique_word_count',\n",
        "       'comment_unique_token_count', 'parent_comment_unique_token_count',\n",
        "       'comment_stopword_count', 'parent_comment_stopword_count',\n",
        "       'comment_mean_word_length', 'parent_comment_mean_word_length',\n",
        "       'comment_mean_token_length', 'parent_comment_mean_token_length',\n",
        "       'comment_char_count', 'parent_comment_char_count',\n",
        "       'comment_punctuation_count', 'parent_comment_punctuation_count',\n",
        "       'comment_hashtag_count', 'parent_comment_hashtag_count',\n",
        "       'comment_number_count', 'parent_comment_number_count',\n",
        "       'weighted_parent_sentiment_score_neutral',\n",
        "       'weighted_parent_sentiment_score_positive',\n",
        "       'weighted_comment_sentiment_score_neutral',\n",
        "       'weighted_comment_sentiment_score_positive']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will standardize columns with boolean values. We will change from boolean to integer type and convert the features from a dense matrix to a sparse matrix using the function csr_matrix. We use MaxAbsScaler() to scale all data down to between -1 and 1 for easy analysis."
      ],
      "metadata": {
        "id": "Qq1bcWoMnDUN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgWZdSmFlv8o"
      },
      "outputs": [],
      "source": [
        "bool_cols = ['weighted_parent_sentiment_score_neutral',\n",
        "             'weighted_parent_sentiment_score_positive',\n",
        "             'weighted_comment_sentiment_score_neutral',\n",
        "             'weighted_comment_sentiment_score_positive']\n",
        "\n",
        "for col in bool_cols: #need to convert bool type to integer\n",
        "    train[col] = train[col].astype(int)\n",
        "    test[col] = test[col].astype(int)\n",
        "\n",
        "X_train_gen_features = csr_matrix(train[list_of_features])\n",
        "\n",
        "#Scaling Non-BoW/TDidf Features\n",
        "X_train_scaler = MaxAbsScaler()\n",
        "X_train_gen_features = X_train_scaler.fit_transform(X_train_gen_features)\n",
        "\n",
        "X_test_gen_features = csr_matrix(test[list_of_features])\n",
        "X_test_gen_features = X_train_scaler.transform(X_test_gen_features)\n",
        "\n",
        "y_train_LR = train['label']\n",
        "y_test_LR = test['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qflUELt5lv8o"
      },
      "source": [
        "### Baseline 1: General Features\n",
        "\n",
        "Performing Logistic Regression using general features as the baseline.\n",
        "\n",
        "We will first train the training data using the LogisticRegression() model. We will then make predictions and compare the actual values and predicted values using the mean and stardard deviation of the ROC_AUC curve. By looking at the difference in the area under curve measurements we can evaluate the accuracy and precision of logistic regression for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uA2xg7Qlv8p"
      },
      "outputs": [],
      "source": [
        "#Data Specific Prep\n",
        "X_train_LR = X_train_gen_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G30OSgC7lv8p",
        "outputId": "c418037e-01e3-4aec-9229-9d94f6cac914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean roc_auc: 0.5408477287886091\n",
            "Standard Deviation of roc_auc: 0.002760830575713631\n"
          ]
        }
      ],
      "source": [
        "k = 5\n",
        "\n",
        "model = LogisticRegression(max_iter = 10000)\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "cross_val_scores = []\n",
        "for train_index, val_index in kf.split(X_train_LR):\n",
        "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
        "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
        "\n",
        "    # Train the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation data\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate roc_auc and store it in the list\n",
        "    roc_auc = roc_auc_score(y_val, y_pred)\n",
        "    cross_val_scores.append(roc_auc)\n",
        "\n",
        "mean_roc_auc = sum(cross_val_scores) / k\n",
        "std_roc_auc = np.std(cross_val_scores)\n",
        "\n",
        "print(f\"Mean roc_auc: {mean_roc_auc}\")\n",
        "print(f\"Standard Deviation of roc_auc: {std_roc_auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-2fPIcolv8p"
      },
      "source": [
        "### Baseline 2: Comment BoW\n",
        "\n",
        "Performing Logistic Regression using Comment Bag of Words as the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BzF3HUPlv8p"
      },
      "outputs": [],
      "source": [
        "#Data Specific Prep\n",
        "X_train_LR = train_bow_csr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjt2aFNtlv8q",
        "outputId": "e3d91898-24dc-42d2-b224-b77e9469ae90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean roc_auc: 0.6657491893956433\n",
            "Standard Deviation of roc_auc: 0.0015035735122413102\n"
          ]
        }
      ],
      "source": [
        "k = 5\n",
        "\n",
        "model = LogisticRegression(max_iter = 10000)\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "cross_val_scores = []\n",
        "for train_index, val_index in kf.split(X_train_LR):\n",
        "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
        "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
        "\n",
        "    # Train the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation data\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate roc_auc and store it in the list\n",
        "    roc_auc = roc_auc_score(y_val, y_pred)\n",
        "    cross_val_scores.append(roc_auc)\n",
        "\n",
        "mean_roc_auc = sum(cross_val_scores) / k\n",
        "std_roc_auc = np.std(cross_val_scores)\n",
        "\n",
        "print(f\"Mean roc_auc: {mean_roc_auc}\")\n",
        "#unigram only: Mean roc_auc: 0.655919472075098\n",
        "#unigram only: Standard Deviation of roc_auc: 0.0008334994867002629\n",
        "print(f\"Standard Deviation of roc_auc: {std_roc_auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTjwiEg4lv8q"
      },
      "source": [
        "### Baseline 3: Comment TD-IDF\n",
        "\n",
        "Performing Logistic Regression using Comment TD-idf as the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxtQRrB3lv8q"
      },
      "outputs": [],
      "source": [
        "#Data Specific Prep\n",
        "X_train_LR = train_tdidf_csr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3Q_phIilv8q",
        "outputId": "6ac73dc0-a36d-4a87-c61e-fa0e4d8676aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean roc_auc: 0.6716594638809426\n",
            "Standard Deviation of roc_auc: 0.0029331962362192585\n"
          ]
        }
      ],
      "source": [
        "k = 5\n",
        "\n",
        "model = LogisticRegression(max_iter = 100000)\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "cross_val_scores = []\n",
        "for train_index, val_index in kf.split(X_train_LR):\n",
        "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
        "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
        "\n",
        "    # Train the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation data\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate roc_auc and store it in the list\n",
        "    roc_auc = roc_auc_score(y_val, y_pred)\n",
        "    cross_val_scores.append(roc_auc)\n",
        "\n",
        "mean_roc_auc = sum(cross_val_scores) / k\n",
        "std_roc_auc = np.std(cross_val_scores)\n",
        "\n",
        "print(f\"Mean roc_auc: {mean_roc_auc}\")\n",
        "print(f\"Standard Deviation of roc_auc: {std_roc_auc}\")\n",
        "#unigram only: Mean roc_auc: 0.6592838234414328\n",
        "#unigram only: Standard Deviation of roc_auc: 0.0009248592313210024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDUj7WF_lv8r"
      },
      "source": [
        "### Baseline 4: General Features + BoW\n",
        "\n",
        "Performing Logistic Regression using both General Features and Comment Bag of Words as the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy10A8P7lv8r"
      },
      "outputs": [],
      "source": [
        "X_train_LR = hstack([X_train_gen_features,train_bow_csr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bO9g59Slv8r",
        "outputId": "b2d54f8d-d311-44c9-e182-fa26c77d0891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean roc_auc: 0.6678710302062921\n",
            "Standard Deviation of roc_auc: 0.0011055746273387122\n"
          ]
        }
      ],
      "source": [
        "k = 5\n",
        "\n",
        "model = LogisticRegression(max_iter = 100000)\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "cross_val_scores = []\n",
        "for train_index, val_index in kf.split(X_train_LR):\n",
        "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
        "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
        "\n",
        "    # Train the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation data\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate roc_auc and store it in the list\n",
        "    roc_auc = roc_auc_score(y_val, y_pred)\n",
        "    cross_val_scores.append(roc_auc)\n",
        "\n",
        "mean_roc_auc = sum(cross_val_scores) / k\n",
        "std_roc_auc = np.std(cross_val_scores)\n",
        "\n",
        "print(f\"Mean roc_auc: {mean_roc_auc}\")\n",
        "print(f\"Standard Deviation of roc_auc: {std_roc_auc}\")\n",
        "#unigram: Mean roc_auc: 0.6578548281567246\n",
        "#unigram: Standard Deviation of roc_auc: 0.0015689875036237729"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXheBB2Ilv8r"
      },
      "source": [
        "### Baseline 5: General Features + TD-IDF\n",
        "\n",
        "Performing Logistic Regression using both General Features and TD-idf as the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79HJz5-1lv8r"
      },
      "outputs": [],
      "source": [
        "X_train_LR = hstack([X_train_gen_features,train_tdidf_csr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjSgafSHlv8s",
        "outputId": "3bd02654-4738-4622-99ab-4f4b8be6d9d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean roc_auc: 0.6736698276888735\n",
            "Standard Deviation of roc_auc: 0.002621238988592881\n"
          ]
        }
      ],
      "source": [
        "k = 5\n",
        "\n",
        "model = LogisticRegression(max_iter = 100000)\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "cross_val_scores = []\n",
        "for train_index, val_index in kf.split(X_train_LR):\n",
        "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
        "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
        "\n",
        "    # Train the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation data\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate roc_auc and store it in the list\n",
        "    roc_auc = roc_auc_score(y_val, y_pred)\n",
        "    cross_val_scores.append(roc_auc)\n",
        "\n",
        "mean_roc_auc = sum(cross_val_scores) / k\n",
        "std_roc_auc = np.std(cross_val_scores)\n",
        "\n",
        "print(f\"Mean roc_auc: {mean_roc_auc}\")\n",
        "print(f\"Standard Deviation of roc_auc: {std_roc_auc}\")\n",
        "#unigram: Mean roc_auc: 0.6623742530514969\n",
        "#unigram: Standard Deviation of roc_auc: 0.0010939794906847484"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc7lmC1flv8s"
      },
      "source": [
        "### Baseline 6: General Features + Comment TDIDF + Parent TDIDF\n",
        "\n",
        "Performing Logistic Regression using General Features, Comment TD-idf and Parent comment TD-idf as the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXwA4EMplv8s"
      },
      "outputs": [],
      "source": [
        "X_train_LR = hstack([X_train_gen_features,train_tdidf_csr,train_parent_tdidf_csr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEMZlrQHlv8s",
        "outputId": "17a9e572-4576-42c3-d5a7-934f7fe9f42a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean roc_auc: 0.6688676251297716\n",
            "Standard Deviation of roc_auc: 0.002473666408773878\n"
          ]
        }
      ],
      "source": [
        "k = 5\n",
        "\n",
        "model = LogisticRegression(max_iter = 100000)\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "cross_val_scores = []\n",
        "for train_index, val_index in kf.split(X_train_LR):\n",
        "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
        "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
        "\n",
        "    # Train the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation data\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate roc_auc and store it in the list\n",
        "    roc_auc = roc_auc_score(y_val, y_pred)\n",
        "    cross_val_scores.append(roc_auc)\n",
        "\n",
        "mean_roc_auc = sum(cross_val_scores) / k\n",
        "std_roc_auc = np.std(cross_val_scores)\n",
        "\n",
        "print(f\"Mean roc_auc: {mean_roc_auc}\")\n",
        "print(f\"Standard Deviation of roc_auc: {std_roc_auc}\")\n",
        "#unigram: Mean roc_auc: 0.6595766552618814\n",
        "#unigram: Standard Deviation of roc_auc: 0.0020504330756799696"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmEXFgYklv8s"
      },
      "source": [
        "### Baseline 7: General Features + Parent TDIDF\n",
        "\n",
        "Performing Logistic Regression using both General Features and Parent comment TD-idf as the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kpSYwzGlv8s"
      },
      "outputs": [],
      "source": [
        "X_train_LR = hstack([X_train_gen_features,train_parent_tdidf_csr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5f9f_JClv8t",
        "outputId": "4c2c3a4b-8cf0-4185-9cae-98431707eeaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean roc_auc: 0.5674453786008516\n",
            "Standard Deviation of roc_auc: 0.0029968450288943827\n"
          ]
        }
      ],
      "source": [
        "k = 5\n",
        "\n",
        "model = LogisticRegression(max_iter = 100000)\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "cross_val_scores = []\n",
        "for train_index, val_index in kf.split(X_train_LR):\n",
        "    X_train, X_val = X_train_LR[train_index], X_train_LR[val_index]\n",
        "    y_train, y_val = y_train_LR.iloc[train_index,], y_train_LR.iloc[val_index,]\n",
        "\n",
        "    # Train the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation data\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate roc_auc and store it in the list\n",
        "    roc_auc = roc_auc_score(y_val, y_pred)\n",
        "    cross_val_scores.append(roc_auc)\n",
        "\n",
        "mean_roc_auc = sum(cross_val_scores) / k\n",
        "std_roc_auc = np.std(cross_val_scores)\n",
        "\n",
        "print(f\"Mean roc_auc: {mean_roc_auc}\")\n",
        "print(f\"Standard Deviation of roc_auc: {std_roc_auc}\")\n",
        "#unigram: Mean roc_auc: 0.570165520029702\n",
        "#unigram: Standard Deviation of roc_auc: 0.002821502726381473"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4DiLeAElv8t"
      },
      "source": [
        "### Hyper Parameter Tuning with Grid Search CV\n",
        "\n",
        "Baseline 5: General Features + TD-idf has the highest mean roc_auc score (0.67367). This means that it is the best performing classifier. We will now perform hyperparameter tuning on this to increase the model performance. We will use GridSearchCV to cross validate all possible hyperparameter combinations and pick the best performing hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4B6m7aflv8t"
      },
      "outputs": [],
      "source": [
        "### Using gen features and tdidf\n",
        "X_train_LR = hstack([X_train_gen_features,train_tdidf_csr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP2tUcGOlv8t"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'lbfgs'],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZOfBlwhlv8u",
        "outputId": "8a3d8616-43eb-445a-9e90-2610eeacc998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "[CV 1/5] END C=0.001, penalty=l1, solver=liblinear;, score=0.529 total time=   0.1s\n",
            "[CV 2/5] END C=0.001, penalty=l1, solver=liblinear;, score=0.530 total time=   0.1s\n",
            "[CV 3/5] END C=0.001, penalty=l1, solver=liblinear;, score=0.528 total time=   0.1s\n",
            "[CV 4/5] END C=0.001, penalty=l1, solver=liblinear;, score=0.530 total time=   0.1s\n",
            "[CV 5/5] END C=0.001, penalty=l1, solver=liblinear;, score=0.533 total time=   0.1s\n",
            "[CV 1/5] END ...C=0.001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 2/5] END ...C=0.001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 3/5] END ...C=0.001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 4/5] END ...C=0.001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 5/5] END ...C=0.001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 1/5] END C=0.001, penalty=l2, solver=liblinear;, score=0.593 total time=   0.3s\n",
            "[CV 2/5] END C=0.001, penalty=l2, solver=liblinear;, score=0.595 total time=   0.4s\n",
            "[CV 3/5] END C=0.001, penalty=l2, solver=liblinear;, score=0.594 total time=   0.3s\n",
            "[CV 4/5] END C=0.001, penalty=l2, solver=liblinear;, score=0.591 total time=   0.4s\n",
            "[CV 5/5] END C=0.001, penalty=l2, solver=liblinear;, score=0.601 total time=   0.4s\n",
            "[CV 1/5] END .C=0.001, penalty=l2, solver=lbfgs;, score=0.598 total time=   0.4s\n",
            "[CV 2/5] END .C=0.001, penalty=l2, solver=lbfgs;, score=0.600 total time=   0.3s\n",
            "[CV 3/5] END .C=0.001, penalty=l2, solver=lbfgs;, score=0.600 total time=   0.3s\n",
            "[CV 4/5] END .C=0.001, penalty=l2, solver=lbfgs;, score=0.597 total time=   0.2s\n",
            "[CV 5/5] END .C=0.001, penalty=l2, solver=lbfgs;, score=0.607 total time=   0.3s\n",
            "[CV 1/5] END C=0.01, penalty=l1, solver=liblinear;, score=0.580 total time=   0.3s\n",
            "[CV 2/5] END C=0.01, penalty=l1, solver=liblinear;, score=0.580 total time=   0.3s\n",
            "[CV 3/5] END C=0.01, penalty=l1, solver=liblinear;, score=0.577 total time=   0.3s\n",
            "[CV 4/5] END C=0.01, penalty=l1, solver=liblinear;, score=0.580 total time=   0.3s\n",
            "[CV 5/5] END C=0.01, penalty=l1, solver=liblinear;, score=0.583 total time=   0.3s\n",
            "[CV 1/5] END ....C=0.01, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 2/5] END ....C=0.01, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 3/5] END ....C=0.01, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 4/5] END ....C=0.01, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 5/5] END ....C=0.01, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 1/5] END C=0.01, penalty=l2, solver=liblinear;, score=0.662 total time=   0.5s\n",
            "[CV 2/5] END C=0.01, penalty=l2, solver=liblinear;, score=0.660 total time=   0.6s\n",
            "[CV 3/5] END C=0.01, penalty=l2, solver=liblinear;, score=0.659 total time=   0.6s\n",
            "[CV 4/5] END C=0.01, penalty=l2, solver=liblinear;, score=0.658 total time=   0.5s\n",
            "[CV 5/5] END C=0.01, penalty=l2, solver=liblinear;, score=0.660 total time=   0.5s\n",
            "[CV 1/5] END ..C=0.01, penalty=l2, solver=lbfgs;, score=0.662 total time=   0.5s\n",
            "[CV 2/5] END ..C=0.01, penalty=l2, solver=lbfgs;, score=0.659 total time=   0.5s\n",
            "[CV 3/5] END ..C=0.01, penalty=l2, solver=lbfgs;, score=0.658 total time=   0.5s\n",
            "[CV 4/5] END ..C=0.01, penalty=l2, solver=lbfgs;, score=0.658 total time=   0.5s\n",
            "[CV 5/5] END ..C=0.01, penalty=l2, solver=lbfgs;, score=0.659 total time=   0.5s\n",
            "[CV 1/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.684 total time=   0.4s\n",
            "[CV 2/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.685 total time=   0.4s\n",
            "[CV 3/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.680 total time=   0.4s\n",
            "[CV 4/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.679 total time=   0.4s\n",
            "[CV 5/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.682 total time=   0.4s\n",
            "[CV 1/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 2/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 3/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 4/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 5/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 1/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.725 total time=   0.9s\n",
            "[CV 2/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.721 total time=   0.8s\n",
            "[CV 3/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.719 total time=   0.8s\n",
            "[CV 4/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.720 total time=   0.9s\n",
            "[CV 5/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.720 total time=   1.1s\n",
            "[CV 1/5] END ...C=0.1, penalty=l2, solver=lbfgs;, score=0.725 total time=   1.4s\n",
            "[CV 2/5] END ...C=0.1, penalty=l2, solver=lbfgs;, score=0.721 total time=   1.5s\n",
            "[CV 3/5] END ...C=0.1, penalty=l2, solver=lbfgs;, score=0.719 total time=   1.5s\n",
            "[CV 4/5] END ...C=0.1, penalty=l2, solver=lbfgs;, score=0.720 total time=   1.4s\n",
            "[CV 5/5] END ...C=0.1, penalty=l2, solver=lbfgs;, score=0.720 total time=   1.4s\n",
            "[CV 1/5] END .C=1, penalty=l1, solver=liblinear;, score=0.736 total time=   5.1s\n",
            "[CV 2/5] END .C=1, penalty=l1, solver=liblinear;, score=0.733 total time=   4.9s\n",
            "[CV 3/5] END .C=1, penalty=l1, solver=liblinear;, score=0.730 total time=   6.1s\n",
            "[CV 4/5] END .C=1, penalty=l1, solver=liblinear;, score=0.733 total time=   4.7s\n",
            "[CV 5/5] END .C=1, penalty=l1, solver=liblinear;, score=0.732 total time=   5.1s\n",
            "[CV 1/5] END .......C=1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 2/5] END .......C=1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 3/5] END .......C=1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 4/5] END .......C=1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 5/5] END .......C=1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 1/5] END .C=1, penalty=l2, solver=liblinear;, score=0.740 total time=   1.5s\n",
            "[CV 2/5] END .C=1, penalty=l2, solver=liblinear;, score=0.735 total time=   2.0s\n",
            "[CV 3/5] END .C=1, penalty=l2, solver=liblinear;, score=0.734 total time=   1.4s\n",
            "[CV 4/5] END .C=1, penalty=l2, solver=liblinear;, score=0.736 total time=   1.4s\n",
            "[CV 5/5] END .C=1, penalty=l2, solver=liblinear;, score=0.736 total time=   1.6s\n",
            "[CV 1/5] END .....C=1, penalty=l2, solver=lbfgs;, score=0.740 total time=   4.6s\n",
            "[CV 2/5] END .....C=1, penalty=l2, solver=lbfgs;, score=0.735 total time=   4.5s\n",
            "[CV 3/5] END .....C=1, penalty=l2, solver=lbfgs;, score=0.734 total time=   6.6s\n",
            "[CV 4/5] END .....C=1, penalty=l2, solver=lbfgs;, score=0.736 total time=   4.9s\n",
            "[CV 5/5] END .....C=1, penalty=l2, solver=lbfgs;, score=0.736 total time=   4.2s\n",
            "[CV 1/5] END C=10, penalty=l1, solver=liblinear;, score=0.697 total time=  24.2s\n",
            "[CV 2/5] END C=10, penalty=l1, solver=liblinear;, score=0.690 total time=  17.5s\n",
            "[CV 3/5] END C=10, penalty=l1, solver=liblinear;, score=0.689 total time=  21.2s\n",
            "[CV 4/5] END C=10, penalty=l1, solver=liblinear;, score=0.695 total time=  20.0s\n",
            "[CV 5/5] END C=10, penalty=l1, solver=liblinear;, score=0.693 total time=  20.0s\n",
            "[CV 1/5] END ......C=10, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 2/5] END ......C=10, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 3/5] END ......C=10, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 4/5] END ......C=10, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 5/5] END ......C=10, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 1/5] END C=10, penalty=l2, solver=liblinear;, score=0.718 total time=   2.6s\n",
            "[CV 2/5] END C=10, penalty=l2, solver=liblinear;, score=0.712 total time=   2.7s\n",
            "[CV 3/5] END C=10, penalty=l2, solver=liblinear;, score=0.712 total time=   4.4s\n",
            "[CV 4/5] END C=10, penalty=l2, solver=liblinear;, score=0.716 total time=   4.0s\n",
            "[CV 5/5] END C=10, penalty=l2, solver=liblinear;, score=0.715 total time=   3.4s\n",
            "[CV 1/5] END ....C=10, penalty=l2, solver=lbfgs;, score=0.718 total time=  11.7s\n",
            "[CV 2/5] END ....C=10, penalty=l2, solver=lbfgs;, score=0.712 total time=  12.3s\n",
            "[CV 3/5] END ....C=10, penalty=l2, solver=lbfgs;, score=0.712 total time=  11.6s\n",
            "[CV 4/5] END ....C=10, penalty=l2, solver=lbfgs;, score=0.716 total time=  11.2s\n",
            "[CV 5/5] END ....C=10, penalty=l2, solver=lbfgs;, score=0.715 total time=  11.0s\n",
            "[CV 1/5] END C=100, penalty=l1, solver=liblinear;, score=0.656 total time=  42.4s\n",
            "[CV 2/5] END C=100, penalty=l1, solver=liblinear;, score=0.650 total time=  50.3s\n",
            "[CV 3/5] END C=100, penalty=l1, solver=liblinear;, score=0.649 total time=  45.9s\n",
            "[CV 4/5] END C=100, penalty=l1, solver=liblinear;, score=0.656 total time=  34.2s\n",
            "[CV 5/5] END C=100, penalty=l1, solver=liblinear;, score=0.653 total time=  45.3s\n",
            "[CV 1/5] END .....C=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 2/5] END .....C=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 3/5] END .....C=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 4/5] END .....C=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 5/5] END .....C=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
            "[CV 1/5] END C=100, penalty=l2, solver=liblinear;, score=0.683 total time=  11.3s\n",
            "[CV 2/5] END C=100, penalty=l2, solver=liblinear;, score=0.677 total time=   7.3s\n",
            "[CV 3/5] END C=100, penalty=l2, solver=liblinear;, score=0.677 total time=   9.8s\n",
            "[CV 4/5] END C=100, penalty=l2, solver=liblinear;, score=0.683 total time=  11.1s\n",
            "[CV 5/5] END C=100, penalty=l2, solver=liblinear;, score=0.679 total time=   9.8s\n",
            "[CV 1/5] END ...C=100, penalty=l2, solver=lbfgs;, score=0.683 total time=  26.8s\n",
            "[CV 2/5] END ...C=100, penalty=l2, solver=lbfgs;, score=0.677 total time=  27.2s\n",
            "[CV 3/5] END ...C=100, penalty=l2, solver=lbfgs;, score=0.677 total time=  28.6s\n",
            "[CV 4/5] END ...C=100, penalty=l2, solver=lbfgs;, score=0.683 total time=  27.4s\n",
            "[CV 5/5] END ...C=100, penalty=l2, solver=lbfgs;, score=0.679 total time=  26.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
            "30 fits failed out of a total of 120.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1169, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [0.53004281        nan 0.59477668 0.60024644 0.58006235        nan\n",
            " 0.65960442 0.65918519 0.68190355        nan 0.72099054 0.72098068\n",
            " 0.73282862        nan 0.73616565 0.73616304 0.69271459        nan\n",
            " 0.71450208 0.71450001 0.65279085        nan 0.67989042 0.67995127]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=100000),\n",
              "             param_grid={&#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],\n",
              "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
              "                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;, &#x27;lbfgs&#x27;]},\n",
              "             scoring=&#x27;roc_auc&#x27;, verbose=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=100000),\n",
              "             param_grid={&#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],\n",
              "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
              "                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;, &#x27;lbfgs&#x27;]},\n",
              "             scoring=&#x27;roc_auc&#x27;, verbose=4)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=100000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=100000)</pre></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=100000),\n",
              "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
              "                         'penalty': ['l1', 'l2'],\n",
              "                         'solver': ['liblinear', 'lbfgs']},\n",
              "             scoring='roc_auc', verbose=4)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_search = GridSearchCV(LogisticRegression(max_iter=100000), param_grid, cv=5, scoring='roc_auc', verbose=4)\n",
        "grid_search.fit(X_train_LR, y_train_LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSOpJFMylv8u",
        "outputId": "8ce760fc-dbdd-4c78-b71e-b66f6fb78c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Best Score: 0.7361656463799593\n"
          ]
        }
      ],
      "source": [
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Best score achieved during grid search\n",
        "best_score = grid_search.best_score_\n",
        "print(\"Best Score:\", best_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will test out the model using the best hyperparameters."
      ],
      "metadata": {
        "id": "sCGrdsXMue5_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnBHbsDqlv8u",
        "outputId": "95614755-a899-485c-9845-fb5080b772d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Estimator: LogisticRegression(C=1, max_iter=100000, solver='liblinear')\n"
          ]
        }
      ],
      "source": [
        "# Best estimator (the fitted model with the best parameters)\n",
        "best_estimator = grid_search.best_estimator_\n",
        "print(\"Best Estimator:\", best_estimator)\n",
        "\n",
        "# Results for all parameter combinations\n",
        "cv_results = grid_search.cv_results_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl8-poKClv8v",
        "outputId": "f2559636-7e24-4780-f416-da7e2295a699"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Params</th>\n",
              "      <th>Mean Score</th>\n",
              "      <th>STD</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>C: 1, penalty: l2, solver: liblinear</td>\n",
              "      <td>0.736166</td>\n",
              "      <td>0.002248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>C: 1, penalty: l2, solver: lbfgs</td>\n",
              "      <td>0.736163</td>\n",
              "      <td>0.002247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>C: 1, penalty: l1, solver: liblinear</td>\n",
              "      <td>0.732829</td>\n",
              "      <td>0.001818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>C: 0.1, penalty: l2, solver: liblinear</td>\n",
              "      <td>0.720991</td>\n",
              "      <td>0.002173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>C: 0.1, penalty: l2, solver: lbfgs</td>\n",
              "      <td>0.720981</td>\n",
              "      <td>0.002178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>C: 10, penalty: l2, solver: liblinear</td>\n",
              "      <td>0.714502</td>\n",
              "      <td>0.002480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>C: 10, penalty: l2, solver: lbfgs</td>\n",
              "      <td>0.714500</td>\n",
              "      <td>0.002470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>C: 10, penalty: l1, solver: liblinear</td>\n",
              "      <td>0.692715</td>\n",
              "      <td>0.002934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>C: 0.1, penalty: l1, solver: liblinear</td>\n",
              "      <td>0.681904</td>\n",
              "      <td>0.001977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>C: 100, penalty: l2, solver: lbfgs</td>\n",
              "      <td>0.679951</td>\n",
              "      <td>0.002768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>C: 100, penalty: l2, solver: liblinear</td>\n",
              "      <td>0.679890</td>\n",
              "      <td>0.002762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>C: 0.01, penalty: l2, solver: liblinear</td>\n",
              "      <td>0.659604</td>\n",
              "      <td>0.001380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>C: 0.01, penalty: l2, solver: lbfgs</td>\n",
              "      <td>0.659185</td>\n",
              "      <td>0.001401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>C: 100, penalty: l1, solver: liblinear</td>\n",
              "      <td>0.652791</td>\n",
              "      <td>0.002813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>C: 0.001, penalty: l2, solver: lbfgs</td>\n",
              "      <td>0.600246</td>\n",
              "      <td>0.003776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C: 0.001, penalty: l2, solver: liblinear</td>\n",
              "      <td>0.594777</td>\n",
              "      <td>0.003138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C: 0.01, penalty: l1, solver: liblinear</td>\n",
              "      <td>0.580062</td>\n",
              "      <td>0.001926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C: 0.001, penalty: l1, solver: liblinear</td>\n",
              "      <td>0.530043</td>\n",
              "      <td>0.001633</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      Params  Mean Score       STD\n",
              "14      C: 1, penalty: l2, solver: liblinear    0.736166  0.002248\n",
              "15          C: 1, penalty: l2, solver: lbfgs    0.736163  0.002247\n",
              "12      C: 1, penalty: l1, solver: liblinear    0.732829  0.001818\n",
              "10    C: 0.1, penalty: l2, solver: liblinear    0.720991  0.002173\n",
              "11        C: 0.1, penalty: l2, solver: lbfgs    0.720981  0.002178\n",
              "18     C: 10, penalty: l2, solver: liblinear    0.714502  0.002480\n",
              "19         C: 10, penalty: l2, solver: lbfgs    0.714500  0.002470\n",
              "16     C: 10, penalty: l1, solver: liblinear    0.692715  0.002934\n",
              "8     C: 0.1, penalty: l1, solver: liblinear    0.681904  0.001977\n",
              "23        C: 100, penalty: l2, solver: lbfgs    0.679951  0.002768\n",
              "22    C: 100, penalty: l2, solver: liblinear    0.679890  0.002762\n",
              "6    C: 0.01, penalty: l2, solver: liblinear    0.659604  0.001380\n",
              "7        C: 0.01, penalty: l2, solver: lbfgs    0.659185  0.001401\n",
              "20    C: 100, penalty: l1, solver: liblinear    0.652791  0.002813\n",
              "3       C: 0.001, penalty: l2, solver: lbfgs    0.600246  0.003776\n",
              "2   C: 0.001, penalty: l2, solver: liblinear    0.594777  0.003138\n",
              "4    C: 0.01, penalty: l1, solver: liblinear    0.580062  0.001926\n",
              "0   C: 0.001, penalty: l1, solver: liblinear    0.530043  0.001633"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display CV results as a table\n",
        "mean_test_scores = cv_results['mean_test_score']\n",
        "std_test_scores = cv_results['std_test_score']\n",
        "params = cv_results['params']\n",
        "\n",
        "pd.set_option('display.max_colwidth',None)\n",
        "results_df = pd.DataFrame({'Params':params,'Mean Score':mean_test_scores,'STD':std_test_scores})\n",
        "results_df[\"Params\"] = results_df[\"Params\"].apply(lambda x: ', '.join([f'{key}: {value}' for key, value in x.items()]))\n",
        "results_df.sort_values('STD', inplace=True)\n",
        "results_df.sort_values('Mean Score', ascending=False,inplace=True)\n",
        "results_df.dropna() # Na values when solver is lbfgs and penalty function is l1 since lbfgs does not support l1\n",
        "\n",
        "#for mean_score, std_score, param in zip(mean_test_scores, std_test_scores, params):\n",
        "    #print(f\"Mean Score: {mean_score:.3f} (±{std_score:.3f}) for params: {param}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjpGBqEZlv8v"
      },
      "source": [
        "### Compare to Test Set\n",
        "\n",
        "Here we can see that hyperparameter tuning has increased the ROC_AUC score from 0.67367 to 0.67657."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nai1GruLlv8v"
      },
      "outputs": [],
      "source": [
        "X_test_LR = hstack([X_test_gen_features,test_tdidf_csr])\n",
        "#X_test_LR = scaler.fit_transform(X_test_LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dmdGPsplv8w",
        "outputId": "1ddf7e81-3234-4fc4-a9fa-d11c38b77e8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "roc_auc test: 0.6765735899330518\n"
          ]
        }
      ],
      "source": [
        "y_pred_test = best_estimator.predict(X_test_LR)\n",
        "roc_auc_test = roc_auc_score(y_test_LR, y_pred_test)\n",
        "print(f\"roc_auc test: {roc_auc_test}\")\n",
        "#unigram only: roc_auc test: 0.6626099986586551"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}